{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from arch import arch_model\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import seaborn as sns\n",
    "import re\n",
    "import datetime\n",
    "from typing import Union, List, Callable, Dict\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "pd.options.display.float_format = \"{:,.6f}\".format\n",
    "pd.set_option('display.width', 200)\n",
    "pd.set_option('display.max_columns', 30)\n",
    "\n",
    "PLOT_WIDTH, PLOT_HEIGHT = 12, 8\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import t, norm, kurtosis, skew\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "parent_path = os.path.dirname(os.getcwd()) # Get parent path (if using .ipynb file)\n",
    "# parent_path = os.path.dirname(os.path.dirname(os.path.abspath(__file__))) # Get parent path (if using .py file)\n",
    "os.chdir(parent_path) # Set parent path as working directory (for reading and writing files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Union' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 55\u001b[0m\n\u001b[0;32m     51\u001b[0m         df\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtime_series_to_df\u001b[39m(returns: \u001b[43mUnion\u001b[49m[pd\u001b[38;5;241m.\u001b[39mDataFrame, pd\u001b[38;5;241m.\u001b[39mSeries, List[pd\u001b[38;5;241m.\u001b[39mSeries]], name: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturns\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     56\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;124;03m    Converts returns to a DataFrame if it is a Series or a list of Series.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;124;03m    pd.DataFrame: DataFrame of returns.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(returns, pd\u001b[38;5;241m.\u001b[39mDataFrame):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Union' is not defined"
     ]
    }
   ],
   "source": [
    "# Title: Tools for Financial Data Analysis\n",
    "\n",
    "\n",
    "def read_excel_default(excel_name: str,\n",
    "                       sheet_name: str = None, \n",
    "                       index_col : int = 0,\n",
    "                       parse_dates: bool =True,\n",
    "                       print_sheets: bool = False,\n",
    "                       **kwargs):\n",
    "    \"\"\"\n",
    "    Reads an Excel file and returns a DataFrame with specified options.\n",
    "\n",
    "    Parameters:\n",
    "    excel_name (str): The path to the Excel file.\n",
    "    index_col (int, default=0): Column to use as the row index labels of the DataFrame.\n",
    "    parse_dates (bool, default=True): Boolean to parse dates.\n",
    "    print_sheets (bool, default=False): If True, prints the names and first few rows of all sheets.\n",
    "    sheet_name (str or int, default=None): Name or index of the sheet to read. If None, reads the first sheet.\n",
    "    **kwargs: Additional arguments passed to `pd.read_excel`.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame containing the data from the specified Excel sheet.\n",
    "\n",
    "    Notes:\n",
    "    - If `print_sheets` is True, the function will print the names and first few rows of all sheets and return None.\n",
    "    - The function ensures that the index name is set to 'date' if the index column name is 'date', 'dates' or 'datatime', or if the index contains date-like values.\n",
    "    \"\"\"\n",
    "\n",
    "    if print_sheets:\n",
    "        excel_file = pd.ExcelFile(excel_name)  # Load the Excel file to get sheet names\n",
    "        sheet_names = excel_file.sheet_names\n",
    "        n = 0\n",
    "        while True:\n",
    "            try:\n",
    "                sheet = pd.read_excel(excel_name, sheet_name=n)\n",
    "                print(f'Sheet name: {sheet_names[n]}')\n",
    "                print(\"Columns: \" + \", \".join(list(sheet.columns)))\n",
    "                print(sheet.head(3))\n",
    "                n += 1\n",
    "                print('-' * 70)\n",
    "                print('\\n')\n",
    "            except:\n",
    "                return\n",
    "    sheet_name = 0 if sheet_name is None else sheet_name\n",
    "    df = pd.read_excel(excel_name, index_col=index_col, parse_dates=parse_dates,  sheet_name=sheet_name, **kwargs)\n",
    "    df.columns = [col.lower() for col in df.columns]\n",
    "    if df.index.name is not None:\n",
    "        if df.index.name in ['date', 'dates', 'datetime']:\n",
    "            df.index.name = 'date'\n",
    "    elif isinstance(df.index[0], (datetime.date, datetime.datetime)):\n",
    "        df.index.name = 'date'\n",
    "    return df\n",
    "\n",
    "\n",
    "def time_series_to_df(returns: Union[pd.DataFrame, pd.Series, List[pd.Series]], name: str = \"Returns\"):\n",
    "    \"\"\"\n",
    "    Converts returns to a DataFrame if it is a Series or a list of Series.\n",
    "\n",
    "    Parameters:\n",
    "    returns (pd.DataFrame, pd.Series or List or pd.Series): Time series of returns.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame of returns.\n",
    "    \"\"\"\n",
    "    if isinstance(returns, pd.DataFrame):\n",
    "        returns = returns.copy()\n",
    "    if isinstance(returns, pd.Series):\n",
    "        returns = returns.to_frame()\n",
    "    elif isinstance(returns, list):\n",
    "        returns_list = returns.copy()\n",
    "        returns = pd.DataFrame({})\n",
    "\n",
    "        for series in returns_list:\n",
    "            if isinstance(series, pd.Series):\n",
    "                returns = returns.merge(series, right_index=True, left_index=True, how='outer')\n",
    "            else:\n",
    "                raise TypeError(f'{name} must be either a pd.DataFrame or a list of pd.Series')\n",
    "            \n",
    "    # Convert returns to float\n",
    "    try:\n",
    "        returns = returns.apply(lambda x: x.astype(float))\n",
    "    except ValueError:\n",
    "        print(f'Could not convert {name} to float. Check if there are any non-numeric values')\n",
    "        pass\n",
    "\n",
    "    return returns\n",
    "\n",
    "\n",
    "def fix_dates_index(returns: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Fixes the date index of a DataFrame if it is not in datetime format and convert returns to float.\n",
    "\n",
    "    Parameters:\n",
    "    returns (pd.DataFrame): DataFrame of returns.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with datetime index.\n",
    "    \"\"\"\n",
    "    # Check if 'date' is in the columns and set it as the index\n",
    "\n",
    "    # Set index name to 'date' if appropriate\n",
    "    \n",
    "    if returns.index.name is not None:\n",
    "        if returns.index.name.lower() in ['date', 'dates', 'datetime']:\n",
    "            returns.index.name = 'date'\n",
    "    elif isinstance(returns.index[0], (datetime.date, datetime.datetime)):\n",
    "        returns.index.name = 'date'\n",
    "    elif 'date' in returns.columns.str.lower():\n",
    "        returns = returns.rename({'Date': 'date'}, axis=1)\n",
    "        returns = returns.set_index('date')\n",
    "    elif 'datetime' in returns.columns.str.lower():\n",
    "        returns = returns.rename({'Datetime': 'date'}, axis=1)\n",
    "        returns = returns.rename({'datetime': 'date'}, axis=1)\n",
    "        returns = returns.set_index('date')\n",
    "\n",
    "    # Convert dates to datetime if not already in datetime format or if minutes are 0\n",
    "    try:\n",
    "        returns.index = pd.to_datetime(returns.index, utc=True)\n",
    "    except ValueError:\n",
    "        print('Could not convert the index to datetime. Check the index format for invalid dates.')\n",
    "    if not isinstance(returns.index, pd.DatetimeIndex) or (returns.index.minute == 0).all():\n",
    "        returns.index = pd.to_datetime(returns.index.map(lambda x: x.date()))\n",
    "        \n",
    "    # Convert returns to float\n",
    "    try:\n",
    "        returns = returns.apply(lambda x: x.astype(float))\n",
    "    except ValueError:\n",
    "        print('Could not convert returns to float. Check if there are any non-numeric values')\n",
    "        pass\n",
    "\n",
    "    return returns\n",
    "\n",
    "\n",
    "def filter_columns_and_indexes(\n",
    "    df: pd.DataFrame,\n",
    "    keep_columns: Union[list, str],\n",
    "    drop_columns: Union[list, str],\n",
    "    keep_indexes: Union[list, str],\n",
    "    drop_indexes: Union[list, str]\n",
    "):\n",
    "    \"\"\"\n",
    "    Filters a DataFrame based on specified columns and indexes.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame to be filtered.\n",
    "    keep_columns (list or str): Columns to keep in the DataFrame.\n",
    "    drop_columns (list or str): Columns to drop from the DataFrame.\n",
    "    keep_indexes (list or str): Indexes to keep in the DataFrame.\n",
    "    drop_indexes (list or str): Indexes to drop from the DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The filtered DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    if not isinstance(df, (pd.DataFrame, pd.Series)):\n",
    "        return df\n",
    "    \n",
    "    df = df.copy()\n",
    "\n",
    "    # Columns\n",
    "    if keep_columns is not None:\n",
    "        keep_columns = [re.escape(col) for col in keep_columns]\n",
    "        keep_columns = \"(?i).*(\" + \"|\".join(keep_columns) + \").*\" if isinstance(keep_columns, list) else \"(?i).*\" + keep_columns + \".*\"\n",
    "        df = df.filter(regex=keep_columns)\n",
    "        if drop_columns is not None:\n",
    "            print('Both \"keep_columns\" and \"drop_columns\" were specified. \"drop_columns\" will be ignored.')\n",
    "\n",
    "    elif drop_columns is not None:\n",
    "        drop_columns = [re.escape(col) for col in drop_columns]\n",
    "        drop_columns = \"(?i).*(\" + \"|\".join(drop_columns) + \").*\" if isinstance(drop_columns, list) else \"(?i).*\" + drop_columns + \".*\"\n",
    "        df = df.drop(columns=df.filter(regex=drop_columns).columns)\n",
    "\n",
    "    # Indexes\n",
    "    if keep_indexes is not None:\n",
    "        keep_indexes = [re.escape(col) for col in keep_indexes]\n",
    "        keep_indexes = \"(?i).*(\" + \"|\".join(keep_indexes) + \").*\" if isinstance(keep_indexes, list) else \"(?i).*\" + keep_indexes + \".*\"\n",
    "        df = df.filter(regex=keep_indexes, axis=0)\n",
    "        if drop_indexes is not None:\n",
    "            print('Both \"keep_indexes\" and \"drop_indexes\" were specified. \"drop_indexes\" will be ignored.')\n",
    "\n",
    "    elif drop_indexes is not None:\n",
    "        drop_indexes = [re.escape(col) for col in drop_indexes]\n",
    "        drop_indexes = \"(?i).*(\" + \"|\".join(drop_indexes) + \").*\" if isinstance(drop_indexes, list) else \"(?i).*\" + drop_indexes + \".*\"\n",
    "        df = df.filter(regex=keep_indexes, axis=0)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Returns and Performance Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Union' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot_cumulative_returns\u001b[39m(\n\u001b[1;32m----> 2\u001b[0m     cumulative_returns: \u001b[43mUnion\u001b[49m[pd\u001b[38;5;241m.\u001b[39mDataFrame, pd\u001b[38;5;241m.\u001b[39mSeries],\n\u001b[0;32m      3\u001b[0m     name: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m      4\u001b[0m ):\n\u001b[0;32m      5\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124;03m    Plots cumulative returns from a time series of cumulative returns.\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124;03m    None\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;66;03m# Handle index formatting for hours, minutes, and seconds\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Union' is not defined"
     ]
    }
   ],
   "source": [
    "def plot_cumulative_returns(\n",
    "    cumulative_returns: Union[pd.DataFrame, pd.Series],\n",
    "    name: str = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Plots cumulative returns from a time series of cumulative returns.\n",
    "\n",
    "    Parameters:\n",
    "    cumulative_returns (pd.DataFrame or pd.Series): Time series of cumulative returns.\n",
    "    name (str, default=None): Name for the title of the plot.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    # Handle index formatting for hours, minutes, and seconds\n",
    "    indexes_cum_ret = cumulative_returns.index\n",
    "    if indexes_cum_ret[0].hour == 0 and indexes_cum_ret[0].minute == 0 and indexes_cum_ret[0].second == 0:\n",
    "        formatted_index = indexes_cum_ret.strftime('%Y-%m-%d')\n",
    "    else:\n",
    "        formatted_index = indexes_cum_ret.strftime('%Y-%m-%d\\n%H:%M:%S')\n",
    "\n",
    "    continuous_index = range(len(indexes_cum_ret))\n",
    "\n",
    "    # Plot cumulative returns\n",
    "    fig, ax = plt.subplots(figsize=(PLOT_WIDTH, PLOT_HEIGHT))\n",
    "    if isinstance(cumulative_returns, pd.Series):\n",
    "        ax.plot(continuous_index, cumulative_returns, label='Cumulative Returns', linewidth=1.5, color='blue')\n",
    "    elif isinstance(cumulative_returns, pd.DataFrame):\n",
    "        for column in cumulative_returns.columns:\n",
    "            ax.plot(continuous_index, cumulative_returns[column], label=column, linewidth=1.5)\n",
    "    else:\n",
    "        raise ValueError(\"`cumulative_returns` must be a pandas DataFrame or Series.\")\n",
    "\n",
    "    # Format x-axis with formatted dates\n",
    "    num_ticks = 20\n",
    "    tick_indices = np.linspace(0, len(continuous_index) - 1, num=num_ticks, dtype=int)\n",
    "    tick_labels = [formatted_index[i] for i in tick_indices]\n",
    "    ax.set_xticks(tick_indices)\n",
    "    ax.set_xticklabels(tick_labels, rotation=45, fontsize=8)\n",
    "    ax.set_xlim([0, len(formatted_index) - 1])\n",
    "\n",
    "    # Add percentage formatting for y-axis\n",
    "    ax.yaxis.set_major_formatter(mticker.PercentFormatter(xmax=1, decimals=2))\n",
    "\n",
    "    # Add zero line\n",
    "    ax.axhline(0, color='darkgrey', linewidth=1, linestyle='-')\n",
    "    ax.set_title(f'Cumulative Returns {name}' if name else 'Cumulative Returns', fontsize=14)\n",
    "\n",
    "    # Style grid and background\n",
    "    ax.grid(True, linestyle='--', linewidth=0.5)\n",
    "    ax.legend(fontsize=10)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def calc_cumulative_returns(\n",
    "    returns: Union[pd.DataFrame, pd.Series, List[pd.Series]],\n",
    "    plot_returns: bool = True,\n",
    "    name: str = None,\n",
    "    return_series: bool = True,\n",
    "    timeframes: Union[None, dict] = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculates cumulative returns from a time series of returns.\n",
    "\n",
    "    Parameters:\n",
    "    returns (pd.DataFrame, pd.Series or List or pd.Series): Time series of returns.\n",
    "    plot_returns (bool, default=True): If True, plots the cumulative returns.\n",
    "    name (str, default=None): Name for the cumulative return series.\n",
    "    return_series (bool, default=True): If True, returns the cumulative returns as a Series.\n",
    "    timeframes (dict or None, default=None): Dictionary of timeframes to calculate cumulative returns for each period.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: Returns cumulative returns DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    returns = time_series_to_df(returns)  # Convert returns to DataFrame if it is a Series or a list of Series\n",
    "    fix_dates_index(returns)  # Fix the date index of the DataFrame if it is not in datetime format and convert returns to float\n",
    "\n",
    "    if timeframes is not None:\n",
    "        results = {}\n",
    "        for name, timeframe in timeframes.items():\n",
    "            if timeframe[0] and timeframe[1]:\n",
    "                timeframe_returns = returns.loc[timeframe[0]:timeframe[1]]\n",
    "            elif timeframe[0]:\n",
    "                timeframe_returns = returns.loc[timeframe[0]:]\n",
    "            elif timeframe[1]:\n",
    "                timeframe_returns = returns.loc[:timeframe[1]]\n",
    "            else:\n",
    "                timeframe_returns = returns.copy()\n",
    "\n",
    "            if len(timeframe_returns.index) == 0:\n",
    "                raise Exception(f'No returns data for {name} timeframe')\n",
    "\n",
    "            cumulative_returns = calc_cumulative_returns(\n",
    "                timeframe_returns,\n",
    "                return_series=True,\n",
    "                plot_returns=plot_returns,\n",
    "                name=name,\n",
    "                timeframes=None\n",
    "            )\n",
    "            results[name] = cumulative_returns\n",
    "        return results\n",
    "\n",
    "    cumulative_returns = (1 + returns).cumprod() - 1\n",
    "\n",
    "    if plot_returns:\n",
    "        plot_cumulative_returns(cumulative_returns, name=name)\n",
    "\n",
    "    if return_series:\n",
    "        return cumulative_returns\n",
    "    \n",
    "\n",
    "def calc_returns_statistics(\n",
    "    returns: Union[pd.DataFrame, pd.Series, List[pd.Series]],\n",
    "    annual_factor: int = None,\n",
    "    provided_excess_returns: bool = None,\n",
    "    rf_returns: Union[pd.Series, pd.DataFrame] = None,\n",
    "    var_quantile: Union[float , List] = .05,\n",
    "    timeframes: Union[None, dict] = None,\n",
    "    return_tangency_weights: bool = False,\n",
    "    correlations: Union[bool, List] = False,\n",
    "    tail_risks: bool = True,\n",
    "    keep_columns: Union[list, str] = None,\n",
    "    drop_columns: Union[list, str] = None,\n",
    "    keep_indexes: Union[list, str] = None,\n",
    "    drop_indexes: Union[list, str] = None,\n",
    "    _timeframe_name: str = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculates summary statistics for a time series of returns.   \n",
    "\n",
    "    Parameters:\n",
    "    returns (pd.DataFrame, pd.Series or List or pd.Series): Time series of returns.\n",
    "    annual_factor (int, default=None): Factor for annualizing returns.\n",
    "    provided_excess_returns (bool, default=None): Whether excess returns are already provided.\n",
    "    rf (pd.Series or pd.DataFrame, default=None): Risk-free rate data.\n",
    "    var_quantile (float or list, default=0.05): Quantile for Value at Risk (VaR) calculation.\n",
    "    timeframes (dict or None, default=None): Dictionary of timeframes [start, finish] to calculate statistics for each period.\n",
    "    return_tangency_weights (bool, default=True): If True, returns tangency portfolio weights.\n",
    "    correlations (bool or list, default=True): If True, returns correlations, or specify columns for correlations.\n",
    "    tail_risks (bool, default=True): If True, include tail risk statistics.\n",
    "    keep_columns (list or str, default=None): Columns to keep in the resulting DataFrame.\n",
    "    drop_columns (list or str, default=None): Columns to drop from the resulting DataFrame.\n",
    "    keep_indexes (list or str, default=None): Indexes to keep in the resulting DataFrame.\n",
    "    drop_indexes (list or str, default=None): Indexes to drop from the resulting DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Summary statistics of the returns.\n",
    "    \"\"\"\n",
    "\n",
    "    returns = time_series_to_df(returns) # Convert returns to DataFrame if it is a Series or a list of Series\n",
    "    fix_dates_index(returns) # Fix the date index of the DataFrame if it is not in datetime format and convert returns to float\n",
    "\n",
    "    if rf_returns is not None:\n",
    "        rf_returns = time_series_to_df(rf_returns) # Convert returns to DataFrame if it is a Series\n",
    "        fix_dates_index(rf_returns) # Fix the date index of the DataFrame if it is not in datetime format and convert returns to float\n",
    "        rf_returns = rf_returns.reindex(returns.index).dropna()\n",
    "        \n",
    "        if len(rf_returns.index) != len(returns.index):\n",
    "            raise Exception('\"rf_returns\" has missing data to match \"returns\" index')\n",
    "        if type(rf_returns) == pd.DataFrame:\n",
    "            rf = rf_returns.iloc[:, 0].to_list()\n",
    "        elif type(rf_returns) == pd.Series:\n",
    "            rf = rf_returns.to_list()\n",
    "\n",
    "    # Assume annualization factor of 12 for monthly returns if None and notify user\n",
    "    if annual_factor is None:\n",
    "        print('Assuming monthly returns with annualization term of 12')\n",
    "        annual_factor = 12\n",
    "\n",
    "    \n",
    "    if keep_columns is None:\n",
    "        keep_columns = ['Accumulated Return', 'Annualized Mean', 'Annualized Vol', 'Annualized Sharpe', 'Min', 'Mean', 'Max', 'Correlation']\n",
    "        if tail_risks == True:\n",
    "            keep_columns += ['Skewness', 'Excess Kurtosis', f'Historical VaR', f'Annualized Historical VaR', \n",
    "                                f'Historical CVaR', f'Annualized Historical CVaR', 'Max Drawdown', \n",
    "                                'Peak Date', 'Bottom Date', 'Recovery', 'Duration (days)']\n",
    "    if return_tangency_weights == True:\n",
    "        keep_columns += ['Tangency Portfolio']\n",
    "    if correlations != False:\n",
    "        keep_columns += ['Correlation']\n",
    "\n",
    "    # Iterate to calculate statistics for multiple timeframes\n",
    "    if isinstance(timeframes, dict):\n",
    "        all_timeframes_summary_statistics = pd.DataFrame({})\n",
    "        for name, timeframe in timeframes.items():\n",
    "            if timeframe[0] and timeframe[1]:\n",
    "                timeframe_returns = returns.loc[timeframe[0]:timeframe[1]]\n",
    "            elif timeframe[0]:\n",
    "                timeframe_returns = returns.loc[timeframe[0]:]\n",
    "            elif timeframe[1]:\n",
    "                timeframe_returns = returns.loc[:timeframe[1]]\n",
    "            else:\n",
    "                timeframe_returns = returns.copy()\n",
    "            if len(timeframe_returns.index) == 0:\n",
    "                raise Exception(f'No returns data for {name} timeframe')\n",
    "            \n",
    "            timeframe_returns = timeframe_returns.rename(columns=lambda col: col + f' ({name})')\n",
    "            timeframe_summary_statistics = calc_returns_statistics(\n",
    "                returns=timeframe_returns,\n",
    "                annual_factor=annual_factor,\n",
    "                provided_excess_returns=provided_excess_returns,\n",
    "                rf_returns=rf_returns,\n",
    "                var_quantile=var_quantile,\n",
    "                timeframes=None,\n",
    "                return_tangency_weights=return_tangency_weights,\n",
    "                correlations=correlations,\n",
    "                tail_risks=tail_risks,\n",
    "                _timeframe_name=name,\n",
    "                keep_columns=keep_columns,\n",
    "                drop_columns=drop_columns,\n",
    "                keep_indexes=keep_indexes,\n",
    "                drop_indexes=drop_indexes\n",
    "            )\n",
    "            all_timeframes_summary_statistics = pd.concat(\n",
    "                [all_timeframes_summary_statistics, timeframe_summary_statistics],\n",
    "                axis=0\n",
    "            )\n",
    "        return all_timeframes_summary_statistics\n",
    "\n",
    "    # Calculate summary statistics for a single timeframe\n",
    "    summary_statistics = pd.DataFrame(index=returns.columns)\n",
    "    summary_statistics['Mean'] = returns.mean()\n",
    "    summary_statistics['Annualized Mean'] = returns.mean() * annual_factor\n",
    "    summary_statistics['Vol'] = returns.std()\n",
    "    summary_statistics['Annualized Vol'] = returns.std() * np.sqrt(annual_factor)\n",
    "    if provided_excess_returns is True:\n",
    "        if rf_returns is not None:\n",
    "            print('Excess returns and risk-free were both provided.'\n",
    "                ' Excess returns will be consider as is, and risk-free rate given will be ignored.\\n')\n",
    "        summary_statistics['Sharpe'] = returns.mean() / returns.std()\n",
    "    else:\n",
    "        try:\n",
    "            if rf_returns is None:\n",
    "                print('No risk-free rate provided. Interpret \"Sharpe\" as \"Mean/Volatility\".\\n')\n",
    "                summary_statistics['Sharpe'] = returns.mean() / returns.std()\n",
    "            else:\n",
    "                excess_returns = returns.subtract(rf_returns.iloc[:, 0], axis=0)\n",
    "                summary_statistics['Sharpe'] = excess_returns.mean() / returns.std()\n",
    "        except Exception as e:\n",
    "            print(f'Could not calculate Sharpe: {e}')\n",
    "\n",
    "    summary_statistics['Annualized Sharpe'] = summary_statistics['Sharpe'] * np.sqrt(annual_factor)\n",
    "    summary_statistics['Min'] = returns.min()\n",
    "    summary_statistics['Max'] = returns.max()\n",
    "\n",
    "    summary_statistics['Win Rate'] = (returns > 0).mean()\n",
    "    \n",
    "    if tail_risks == True:\n",
    "        tail_risk_stats = stats_tail_risk(returns,\n",
    "                                        annual_factor=annual_factor,\n",
    "                                        var_quantile=var_quantile,\n",
    "                                        keep_indexes=keep_indexes,\n",
    "                                        drop_indexes=drop_indexes)\n",
    "        \n",
    "        summary_statistics = summary_statistics.join(tail_risk_stats)\n",
    "        \n",
    "    if return_tangency_weights is True:\n",
    "        tangency_weights = calc_tangency_port(returns, name = 'Tangency')\n",
    "        summary_statistics = summary_statistics.join(tangency_weights)\n",
    "\n",
    "    if correlations is True or isinstance(correlations, list):\n",
    "               \n",
    "        returns_corr = returns.corr()\n",
    "        if _timeframe_name:\n",
    "            returns_corr = returns_corr.rename(columns=lambda col: col.replace(f' ({_timeframe_name})', ''))\n",
    "\n",
    "        if isinstance(correlations, list):\n",
    "            # Check if all selected columns exist in returns_corr\n",
    "            corr_not_in_returns_corr = [col for col in correlations if col not in returns_corr.columns]\n",
    "            if len(corr_not_in_returns_corr) > 0:\n",
    "                not_in_returns_corr = \", \".join([c for c in corr_not_in_returns_corr])\n",
    "                raise Exception(f'{not_in_returns_corr} not in returns columns')\n",
    "            \n",
    "            returns_corr = returns_corr[[col for col in correlations]]\n",
    "            \n",
    "        returns_corr = returns_corr.rename(columns=lambda col: col + ' Correlation')\n",
    "        \n",
    "        # Specify a suffix to be added to overlapping columns\n",
    "        summary_statistics = summary_statistics.join(returns_corr)\n",
    "    \n",
    "    return filter_columns_and_indexes(\n",
    "        summary_statistics,\n",
    "        keep_columns=keep_columns,\n",
    "        drop_columns=drop_columns,\n",
    "        keep_indexes=keep_indexes,\n",
    "        drop_indexes=drop_indexes\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tail Risks, Covariances and Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Union' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstats_tail_risk\u001b[39m(\n\u001b[1;32m----> 2\u001b[0m     returns: \u001b[43mUnion\u001b[49m[pd\u001b[38;5;241m.\u001b[39mDataFrame, pd\u001b[38;5;241m.\u001b[39mSeries, List[pd\u001b[38;5;241m.\u001b[39mSeries]],\n\u001b[0;32m      3\u001b[0m     annual_factor: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m      4\u001b[0m     var_quantile: Union[\u001b[38;5;28mfloat\u001b[39m , List] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m.05\u001b[39m,\n\u001b[0;32m      5\u001b[0m     keep_columns: Union[\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m      6\u001b[0m     drop_columns: Union[\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m      7\u001b[0m     keep_indexes: Union[\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m      8\u001b[0m     drop_indexes: Union[\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m      9\u001b[0m ):\n\u001b[0;32m     10\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03m    Calculates tail risk summary statistics for a time series of returns.   \u001b[39;00m\n\u001b[0;32m     12\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;124;03m    pd.DataFrame: tail risk summary statistics of the returns.\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m     26\u001b[0m     returns \u001b[38;5;241m=\u001b[39m time_series_to_df(returns) \u001b[38;5;66;03m# Convert returns to DataFrame if it is a Series or a list of Series\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Union' is not defined"
     ]
    }
   ],
   "source": [
    "def stats_tail_risk(\n",
    "    returns: Union[pd.DataFrame, pd.Series, List[pd.Series]],\n",
    "    annual_factor: int = None,\n",
    "    var_quantile: Union[float , List] = .05,\n",
    "    keep_columns: Union[list, str] = None,\n",
    "    drop_columns: Union[list, str] = None,\n",
    "    keep_indexes: Union[list, str] = None,\n",
    "    drop_indexes: Union[list, str] = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculates tail risk summary statistics for a time series of returns.   \n",
    "\n",
    "    Parameters:\n",
    "    returns (pd.DataFrame, pd.Series or List or pd.Series): Time series of returns.\n",
    "    annual_factor (int, default=None): Factor for annualizing returns.\n",
    "    var_quantile (float or list, default=0.05): Quantile for Value at Risk (VaR) calculation.\n",
    "    keep_columns (list or str, default=None): Columns to keep in the resulting DataFrame.\n",
    "    drop_columns (list or str, default=None): Columns to drop from the resulting DataFrame.\n",
    "    keep_indexes (list or str, default=None): Indexes to keep in the resulting DataFrame.\n",
    "    drop_indexes (list or str, default=None): Indexes to drop from the resulting DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: tail risk summary statistics of the returns.\n",
    "    \"\"\"\n",
    "\n",
    "    returns = time_series_to_df(returns) # Convert returns to DataFrame if it is a Series or a list of Series\n",
    "    fix_dates_index(returns) # Fix the date index of the DataFrame if it is not in datetime format and convert returns to float\n",
    "\n",
    "    tail_risk_stats = pd.DataFrame(index=returns.columns)\n",
    "\n",
    "    tail_risk_stats['Skewness'] = returns.skew()\n",
    "    tail_risk_stats['Excess Kurtosis'] = returns.kurtosis()\n",
    "    var_quantile = [var_quantile] if isinstance(var_quantile, (float, int)) else var_quantile\n",
    "    for var_q in var_quantile:\n",
    "        tail_risk_stats[f'Historical VaR ({var_q:.1%})'] = returns.quantile(var_q, axis = 0)\n",
    "        tail_risk_stats[f'Historical CVaR ({var_q:.1%})'] = returns[returns <= returns.quantile(var_q, axis = 0)].mean()\n",
    "        if annual_factor:\n",
    "            tail_risk_stats[f'Annualized Historical VaR ({var_q:.1%})'] = returns.quantile(var_q, axis = 0) * np.sqrt(annual_factor)\n",
    "            tail_risk_stats[f'Annualized Historical CVaR ({var_q:.1%})'] = returns[returns <= returns.quantile(var_q, axis = 0)].mean() * np.sqrt(annual_factor)\n",
    "    \n",
    "    cum_returns = (1 + returns).cumprod()\n",
    "    maximum = cum_returns.cummax()\n",
    "    drawdown = cum_returns / maximum - 1\n",
    "\n",
    "    tail_risk_stats['Accumulated Return'] = cum_returns.iloc[-1] - 1\n",
    "    tail_risk_stats['Max Drawdown'] = drawdown.min()\n",
    "    tail_risk_stats['Peak Date'] = [maximum[col][:drawdown[col].idxmin()].idxmax() for col in maximum.columns]\n",
    "    tail_risk_stats['Bottom Date'] = drawdown.idxmin()\n",
    "    \n",
    "    recovery_date = []\n",
    "    for col in cum_returns.columns:\n",
    "        prev_max = maximum[col][:drawdown[col].idxmin()].max()\n",
    "        recovery_wealth = pd.DataFrame([cum_returns[col][drawdown[col].idxmin():]]).T\n",
    "        recovery_date.append(recovery_wealth[recovery_wealth[col] >= prev_max].index.min())\n",
    "    tail_risk_stats['Recovery'] = recovery_date\n",
    "\n",
    "    tail_risk_stats[\"Duration (days)\"] = [\n",
    "        (i - j).days if i != pd.NaT else \"-\" for i, j in\n",
    "        zip(tail_risk_stats[\"Recovery\"], tail_risk_stats[\"Bottom Date\"])\n",
    "    ]\n",
    "\n",
    "    return filter_columns_and_indexes(\n",
    "        tail_risk_stats,\n",
    "        keep_columns=keep_columns,\n",
    "        drop_columns=drop_columns,\n",
    "        keep_indexes=keep_indexes,\n",
    "        drop_indexes=drop_indexes\n",
    "    )\n",
    "\n",
    "def calc_ewma_volatility(\n",
    "        returns: pd.Series,\n",
    "        theta : float = 0.94,\n",
    "        ewma_initial_annual_vol : float = None,\n",
    "        annual_factor: int = None\n",
    "    ):\n",
    "\n",
    "    if ewma_initial_annual_vol is not None:\n",
    "        if annual_factor is None:\n",
    "            print('Assuming monthly returns with annualization term of 12')\n",
    "            annual_factor = 12\n",
    "        ewma_initial_annual_vol = ewma_initial_annual_vol / np.sqrt(annual_factor)\n",
    "    else:\n",
    "        ewma_initial_annual_vol = returns.std()\n",
    "    \n",
    "    var_t0 = ewma_initial_annual_vol ** 2\n",
    "    ewma_var = [var_t0]\n",
    "    for i in range(len(returns.index)):\n",
    "        new_ewma_var = ewma_var[-1] * theta + (returns.iloc[i] ** 2) * (1 - theta)\n",
    "        ewma_var.append(new_ewma_var)\n",
    "    ewma_var.pop(0) # Remove var_t0\n",
    "    ewma_vol = [np.sqrt(v) for v in ewma_var]\n",
    "    return pd.Series(ewma_vol, index=returns.index)\n",
    "\n",
    "\n",
    "def calc_garch_volatility(\n",
    "        returns: pd.Series,\n",
    "        p: int = 1,\n",
    "        q: int = 1\n",
    "    ):\n",
    "    model = arch_model(returns, vol='Garch', p=p, q=q)\n",
    "    fitted_model = model.fit(disp='off')\n",
    "    fitted_values = fitted_model.conditional_volatility\n",
    "    return pd.Series(fitted_values, index=returns.index)\n",
    "\n",
    "\n",
    "def calc_var_cvar_summary(\n",
    "    returns: Union[pd.Series, pd.DataFrame],\n",
    "    percentile: Union[None, float] = .05,\n",
    "    window: Union[None, str] = None,\n",
    "    return_hit_ratio: bool = False,\n",
    "    filter_first_hit_ratio_date: Union[None, str, datetime.date] = None,\n",
    "    z_score: float = None,\n",
    "    shift: int = 1,\n",
    "    std_formula: bool = False,\n",
    "    ewma_theta : float = .94,\n",
    "    ewma_initial_annual_vol: float = None,\n",
    "    include_garch: bool = False,\n",
    "    garch_p: int = 1,\n",
    "    garch_q: int = 1,\n",
    "    annual_factor: int = None,\n",
    "    return_stats: Union[str, list] = ['Returns', 'VaR', 'CVaR', 'Vol'],\n",
    "    keep_columns: Union[list, str] = None,\n",
    "    drop_columns: Union[list, str] = None,\n",
    "    keep_indexes: Union[list, str] = None,\n",
    "    drop_indexes: Union[list, str] = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculates a summary of VaR (Value at Risk), CVaR (Conditional VaR), kurtosis, and skewness for the provided returns.\n",
    "\n",
    "    Parameters:\n",
    "    returns (pd.Series or pd.DataFrame): Time series of returns.\n",
    "    percentile (float or None, default=0.05): Percentile to calculate the VaR and CVaR.\n",
    "    window (str or None, default=None): Window size for rolling calculations.\n",
    "    return_hit_ratio (bool, default=False): If True, returns the hit ratio for the VaR.\n",
    "    filter_first_hit_ratio_date (str, datetime.date or None, default=None): Date to filter the hit ratio calculation from then on.\n",
    "    z_score (float, default=None): Z-score for parametric VaR calculation, in case no percentile is provided.\n",
    "    shift (int, default=1): Period shift for VaR/CVaR calculations to avoid look-ahead bias.\n",
    "    std_formula (bool, default=False): If True, uses the normal volatility formula with .std(). Else, use squared returns.\n",
    "    ewma_theta (float, default=0.94): Theta parameter for the EWMA volatility calculation.\n",
    "    ewma_initial_annual_vol (float, default=None): Initial annual volatility for the EWMA calculation.\n",
    "    include_garch (bool, default=False): If True, includes GARCH volatility in the summary.\n",
    "    garch_p (int, default=1): Order of the GARCH model.\n",
    "    garch_q (int, default=1): Order of the GARCH model.\n",
    "    annual_factor (int, default=None): Factor for annualizing returns.\n",
    "    return_stats (str or list, default=['Returns', 'VaR', 'CVaR', 'Vol']): Statistics to return in the summary.\n",
    "    keep_columns (list or str, default=None): Columns to keep in the resulting DataFrame.\n",
    "    drop_columns (list or str, default=None): Columns to drop from the resulting DataFrame.\n",
    "    keep_indexes (list or str, default=None): Indexes to keep in the resulting DataFrame.\n",
    "    drop_indexes (list or str, default=None): Indexes to drop from the resulting DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Summary of VaR and CVaR statistics.\n",
    "    \"\"\"\n",
    "    if annual_factor is None:\n",
    "        print('Assuming monthly returns with annualization term of 12')\n",
    "        annual_factor = 12\n",
    "    if window is None:\n",
    "        print('Using \"window\" of 60 periods, since none was specified')\n",
    "        window = 60\n",
    "    if isinstance(returns, pd.DataFrame):\n",
    "        returns_series = returns.iloc[:, 0]\n",
    "        returns_series.index = returns.index\n",
    "        returns = returns_series.copy()\n",
    "    elif isinstance(returns, pd.Series):\n",
    "        returns = returns.copy()\n",
    "    else:\n",
    "        raise TypeError('returns must be either a pd.DataFrame or a pd.Series')\n",
    "\n",
    "    summary = pd.DataFrame({})\n",
    "\n",
    "    # Returns\n",
    "    summary[f'Returns'] = returns\n",
    "\n",
    "    # Kurtosis\n",
    "    summary[f'Expanding Kurtosis'] = returns.expanding(window).apply(lambda x: kurtosis(x, fisher=True, bias=False))\n",
    "    summary[f'Rolling Kurtosis ({window})'] = returns.rolling(window).apply(lambda x: kurtosis(x, fisher=True, bias=False))\n",
    "\n",
    "    # Skewness\n",
    "    summary[f'Expanding Skewness'] = returns.expanding(window).apply(lambda x: skew(x, bias=False))\n",
    "    summary[f'Rolling Skewness ({window})'] = returns.rolling(window).apply(lambda x: skew(x, bias=False))\n",
    "\n",
    "    # VaR\n",
    "    summary[f'Expanding {window} Historical VaR ({percentile:.0%})'] = returns.expanding(min_periods=window).quantile(percentile)\n",
    "    summary[f'Rolling {window} Historical VaR ({percentile:.0%})'] = returns.rolling(window=window).quantile(percentile)\n",
    "    if std_formula:\n",
    "        summary[f'Expanding {window} Volatility'] = returns.expanding(window).std()\n",
    "        summary[f'Rolling {window} Volatility'] = returns.rolling(window).std()\n",
    "    else: # Volaility assuming zero mean returns\n",
    "        summary[f'Expanding {window} Volatility'] = np.sqrt((returns ** 2).expanding(window).mean())\n",
    "        summary[f'Rolling {window} Volatility'] = np.sqrt((returns ** 2).rolling(window).mean())\n",
    "    summary[f'EWMA {ewma_theta:.2f} Volatility'] = calc_ewma_volatility(returns, theta=ewma_theta, ewma_initial_annual_vol=ewma_initial_annual_vol, annual_factor=annual_factor)\n",
    "    if include_garch:\n",
    "        summary[f'GARCH({garch_p:.0f}, {garch_q:.0f}) Volatility'] = calc_garch_volatility(returns, p=garch_p, q=garch_q)\n",
    "    \n",
    "    # Parametric VaR assuming zero mean returns\n",
    "    z_score = norm.ppf(percentile) if z_score is None else z_score\n",
    "    summary[f'Expanding {window} Parametric VaR ({percentile:.0%})'] = summary[f'Expanding {window} Volatility'] * z_score\n",
    "    summary[f'Rolling {window} Parametric VaR ({percentile:.0%})'] = summary[f'Rolling {window} Volatility'] * z_score\n",
    "    summary[f'EWMA {ewma_theta:.2f} Parametric VaR ({percentile:.0%})'] = summary[f'EWMA {ewma_theta:.2f} Volatility'] * z_score\n",
    "    if include_garch:\n",
    "        summary[f'GARCH({garch_p:.0f}, {garch_q:.0f}) Parametric VaR ({percentile:.0%})'] = summary[f'GARCH({garch_p:.0f}, {garch_q:.0f}) Volatility'] * z_score\n",
    "\n",
    "    if return_hit_ratio:\n",
    "        var_stats = [\n",
    "            f'Expanding {window} Historical VaR ({percentile:.0%})',\n",
    "            f'Rolling {window} Historical VaR ({percentile:.0%})',\n",
    "            f'Expanding {window} Parametric VaR ({percentile:.0%})',\n",
    "            f'Rolling {window} Parametric VaR ({percentile:.0%})',\n",
    "            f'EWMA {ewma_theta:.2f} Parametric VaR ({percentile:.0%})'\n",
    "        ]\n",
    "        if include_garch:\n",
    "            var_stats.append(f'GARCH({garch_p:.0f}, {garch_q:.0f}) Parametric VaR ({percentile:.0%})')\n",
    "        \n",
    "        summary_hit_ratio = summary.copy()\n",
    "        summary_hit_ratio[var_stats] = summary_hit_ratio[var_stats].shift()\n",
    "        if filter_first_hit_ratio_date:\n",
    "            if isinstance(filter_first_hit_ratio_date, (datetime.date, datetime.datetime)):\n",
    "                filter_first_hit_ratio_date = filter_first_hit_ratio_date.strftime(\"%Y-%m-%d\")\n",
    "            summary_hit_ratio = summary.loc[filter_first_hit_ratio_date:]\n",
    "        summary_hit_ratio = summary_hit_ratio.dropna(axis=0)\n",
    "        summary_hit_ratio[var_stats] = summary_hit_ratio[var_stats].apply(lambda x: (x - summary['Returns']) > 0)\n",
    "        \n",
    "        hit_ratio = pd.DataFrame(summary_hit_ratio[var_stats].mean(), columns=['Hit Ratio'])\n",
    "        hit_ratio['Hit Ratio Error'] = (hit_ratio['Hit Ratio'] - percentile) / percentile\n",
    "        hit_ratio['Hit Ratio Absolute Error'] = abs(hit_ratio['Hit Ratio Error'])\n",
    "        hit_ratio = hit_ratio.sort_values('Hit Ratio Absolute Error')\n",
    "\n",
    "        if z_score is not None:\n",
    "            hit_ratio = hit_ratio.rename(lambda col: re.sub(r'VaR \\(\\d+%\\)', f'VaR ({z_score:.2f})', col), axis=1) # Rename columns\n",
    "        return filter_columns_and_indexes(\n",
    "            hit_ratio,\n",
    "            keep_columns=keep_columns,\n",
    "            drop_columns=drop_columns,\n",
    "            keep_indexes=keep_indexes,\n",
    "            drop_indexes=drop_indexes\n",
    "        )\n",
    "\n",
    "    # CVaR\n",
    "    summary[f'Expanding {window} Historical CVaR ({percentile:.0%})'] = returns.expanding(window).apply(lambda x: x[x < x.quantile(percentile)].mean())\n",
    "    summary[f'Rolling {window} Historical CVaR ({percentile:.0%})'] = returns.rolling(window).apply(lambda x: x[x < x.quantile(percentile)].mean())\n",
    "    summary[f'Expanding {window} Parametrical CVaR ({percentile:.0%})'] = - norm.pdf(z_score) / percentile * summary[f'Expanding {window} Volatility']\n",
    "    summary[f'Rolling {window} Parametrical CVaR ({percentile:.0%})'] = - norm.pdf(z_score) / percentile * summary[f'Rolling {window} Volatility']\n",
    "    summary[f'EWMA {ewma_theta:.2f} Parametrical CVaR ({percentile:.0%})'] = - norm.pdf(z_score) / percentile * summary[f'EWMA {ewma_theta:.2f} Volatility']\n",
    "    if include_garch:\n",
    "        summary[f'GARCH({garch_p:.0f}, {garch_q:.0f}) Parametrical CVaR ({percentile:.0%})'] = - norm.pdf(z_score) / percentile * summary[f'GARCH({garch_p:.0f}, {garch_q:.0f}) Volatility']\n",
    "\n",
    "    if shift > 0:\n",
    "        shift_columns = [c for c in summary.columns if not bool(re.search(\"returns\", c))]\n",
    "        summary[shift_columns] = summary[shift_columns].shift(shift).dropna()\n",
    "        print(f'VaR and CVaR are given shifted by {shift:0f} period(s).')\n",
    "    else:\n",
    "        print('VaR and CVaR are given in-sample.')\n",
    "\n",
    "    return_stats = [return_stats.lower()] if isinstance(return_stats, str) else [s.lower() for s in return_stats]\n",
    "    return_stats = list(map(lambda x: 'volatility' if x == 'vol' else x, return_stats))\n",
    "    \n",
    "    if z_score is not None:\n",
    "        summary = summary.rename(lambda col: re.sub(r'VaR \\(\\d+%\\)', f'VaR ({z_score:.2f})', col), axis=1)\n",
    "\n",
    "    if return_stats == ['all'] or set(return_stats) == set(['returns', 'var', 'cvar', 'volatility']):\n",
    "        summary = summary.loc[:, lambda df: df.columns.map(lambda c: bool(re.search(r\"\\b\" + r\"\\b|\\b\".join(return_stats) + r\"\\b\", c.lower())))]\n",
    "        \n",
    "    return filter_columns_and_indexes(\n",
    "        summary,\n",
    "        keep_columns=keep_columns,\n",
    "        drop_columns=drop_columns,\n",
    "        keep_indexes=keep_indexes,\n",
    "        drop_indexes=drop_indexes\n",
    "    )\n",
    "\n",
    "\n",
    "def plot_var(\n",
    "        returns: Union[pd.DataFrame, pd.Series],\n",
    "        var: Union[pd.DataFrame, pd.Series, List[pd.Series]],\n",
    "        percentile: Union[None, float] = .05,\n",
    "        figsize: tuple = (PLOT_WIDTH, PLOT_HEIGHT),\n",
    "        limit = True,\n",
    "        colors: Union[list, str] = [\"blue\", \"red\", \"orange\", \"green\", \"purple\", \"black\", \"grey\", \"pink\", \"brown\", \"cyan\", \"magenta\", \"yellow\"],\n",
    "        var_name: str = None,\n",
    "        is_excess_returns: bool = False\n",
    "        ):\n",
    "    \"\"\"\n",
    "    Plots a variance graph with returns and highlights returns < VaR \n",
    "\n",
    "    Parameters:\n",
    "    returns (pd.DataFrame, pd.Series or None): Time series of returns.\n",
    "    var (pd.DataFrame, pd.Series or List or pd.Series): Time series of VaR.\n",
    "    percentile (float or None, default=.05): Percentile to calculate the hit ratio.\n",
    "    limit (bool, default=True): If True, limits the y-axis to the minimum return.\n",
    "    figsize (tuple, default=(PLOT_WIDTH, PLOT_HEIGHT)): Size of the plot.\n",
    "    colors (list or str, default=[\"blue\", \"red\", \"orange\", \"green\", \"purple\", \"black\", \"grey\", \"pink\", \"brown\", \"cyan\", \"magenta\", \"yellow\"]): Colors for the plot.\n",
    "    var_name (str, default='VaR'): Name for the VaR column to be uses\n",
    "    is_excess_returns (bool, default=False): If True, adjust y-axis label accordingly.\n",
    "\n",
    "    \"\"\"\n",
    "    var = time_series_to_df(var, \"VaR\") # Convert returns to DataFrame if it is a Series or a list of Series\n",
    "    fix_dates_index(var) # Fix the date index of the DataFrame if it is not in datetime format and convert returns to float\n",
    "\n",
    "    returns = time_series_to_df(returns, \"Returns\")\n",
    "    fix_dates_index(returns)\n",
    "    returns = pd.merge(returns, var, left_index=True, right_index=True).dropna()\n",
    "    \n",
    "    asset_name = returns.columns[0]\n",
    "    if asset_name == 0:\n",
    "        asset_name = \"Asset\"\n",
    "\n",
    "    if var_name is None:\n",
    "        if isinstance(var, pd.DatFrame):\n",
    "            var_name = var.columns[0]\n",
    "            if var_name == 0:\n",
    "                var_name = \"VaR\"\n",
    "\n",
    "    returns.columns = [asset_name, var_name]\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.axhline(y=0, linestyle='--', color='black', alpha=0.5)\n",
    "\n",
    "    # Plot returns\n",
    "    plt.plot(\n",
    "        returns.index,\n",
    "        returns[asset_name],\n",
    "        color=colors[2],\n",
    "        label=f\"{asset_name} Returns\",\n",
    "        alpha=.2\n",
    "    )\n",
    "\n",
    "    if var.shape[1] == 1:\n",
    "\n",
    "        plt.plot(\n",
    "            returns.index,\n",
    "            returns[var_name],\n",
    "            color=colors[0],\n",
    "            label=var_name\n",
    "        )\n",
    "        excess_returns_surpass_var = (\n",
    "            returns\n",
    "            .dropna()\n",
    "            .loc[lambda df: df[asset_name] < df[var_name]]\n",
    "        )\n",
    "        plt.plot(\n",
    "            excess_returns_surpass_var.index,\n",
    "            excess_returns_surpass_var[asset_name],\n",
    "            linestyle=\"\",\n",
    "            marker=\"o\",\n",
    "            color=colors[1],\n",
    "            label=f\"Return < {var_name}\",\n",
    "            markersize=1.5\n",
    "        )\n",
    "    \n",
    "        if limit:\n",
    "            plt.ylim(min(returns[asset_name]), .01)\n",
    "\n",
    "        hit_ratio = len(excess_returns_surpass_var.index) / len(returns.index)\n",
    "        hit_ratio_error = abs((hit_ratio / percentile) - 1)\n",
    "        plt.title(f\"{var_name} of {asset_name} Returns\")\n",
    "        plt.xlabel(f\"Hit Ratio: {hit_ratio:.2%}; Hit Ratio Error: {hit_ratio_error:.2%}\")\n",
    "        if is_excess_returns:\n",
    "            plt.ylabel(\"Excess Returns\")\n",
    "        else:\n",
    "            plt.ylabel(\"Returns\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    else:\n",
    "        for idx, var_series in enumerate(var.columns):\n",
    "            plt.plot(\n",
    "                returns.index,\n",
    "                returns[var_series],\n",
    "                color=colors[idx],\n",
    "                label=var_series\n",
    "            )\n",
    "\n",
    "        plt.title(f\"VaR of {asset_name} Returns\")\n",
    "        if is_excess_returns:\n",
    "            plt.ylabel(\"Excess Returns\")\n",
    "        else:\n",
    "            plt.ylabel(\"Returns\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "def calc_correlations(\n",
    "    returns: Union[pd.DataFrame, pd.Series, List[pd.Series]],\n",
    "    print_highest_lowest: bool = True,\n",
    "    show_heatmap: bool = True,\n",
    "    return_matrix: bool = False,\n",
    "    keep_columns: Union[list, str] = None,\n",
    "    drop_columns: Union[list, str] = None,\n",
    "    keep_indexes: Union[list, str] = None,\n",
    "    drop_indexes: Union[list, str] = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculates the correlation matrix of the provided returns and optionally prints or visualizes it.\n",
    "\n",
    "    Parameters:\n",
    "    returns (pd.DataFrame, pd.Series or List or pd.Series): Time series of returns.\n",
    "    print_highest_lowest (bool, default=True): If True, prints the highest and lowest correlations.\n",
    "    show_heatmap (bool, default=False): If True, returns a heatmap of the correlation matrix.\n",
    "    keep_columns (list or str, default=None): Columns to keep in the resulting DataFrame.\n",
    "    drop_columns (list or str, default=None): Columns to drop from the resulting DataFrame.\n",
    "    keep_indexes (list or str, default=None): Indexes to keep in the resulting DataFrame.\n",
    "    drop_indexes (list or str, default=None): Indexes to drop from the resulting DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    sns.heatmap or pd.DataFrame: Heatmap of the correlation matrix or the correlation matrix itself.\n",
    "    \"\"\"\n",
    "\n",
    "    returns = time_series_to_df(returns) # Convert returns to DataFrame if it is a Series or a list of Series\n",
    "    fix_dates_index(returns) # Fix the date index of the DataFrame if it is not in datetime format and convert returns to float\n",
    "\n",
    "    returns = filter_columns_and_indexes(\n",
    "        returns,\n",
    "        keep_columns=keep_columns,\n",
    "        drop_columns=drop_columns,\n",
    "        keep_indexes=keep_indexes,\n",
    "        drop_indexes=drop_indexes\n",
    "    )\n",
    "\n",
    "    correlation_matrix = returns.corr()\n",
    "\n",
    "    if print_highest_lowest:\n",
    "        highest_lowest_corr = (\n",
    "            correlation_matrix\n",
    "            .unstack()\n",
    "            .sort_values()\n",
    "            .reset_index()\n",
    "            .set_axis(['asset_1', 'asset_2', 'corr'], axis=1)\n",
    "            .loc[lambda df: df.asset_1 != df.asset_2]\n",
    "        )\n",
    "        highest_corr = highest_lowest_corr.iloc[lambda df: len(df)-1, :]\n",
    "        lowest_corr = highest_lowest_corr.iloc[0, :]\n",
    "        print(f'The highest correlation ({highest_corr[\"corr\"]:.4f}) is between {highest_corr.asset_1} and {highest_corr.asset_2}')\n",
    "        print(f'The lowest correlation ({lowest_corr[\"corr\"]:.4f}) is between {lowest_corr.asset_1} and {lowest_corr.asset_2}')\n",
    "\n",
    "    if show_heatmap == True:\n",
    "        fig, ax = plt.subplots(figsize=(PLOT_WIDTH, PLOT_HEIGHT))\n",
    "        heatmap = sns.heatmap(\n",
    "            correlation_matrix, \n",
    "            xticklabels=correlation_matrix.columns,\n",
    "            yticklabels=correlation_matrix.columns,\n",
    "            annot=True,\n",
    "        )\n",
    "        plt.show()\n",
    "\n",
    "    if return_matrix:\n",
    "        return correlation_matrix\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tangency and Other Portfolios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Union' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalc_tangency_port\u001b[39m(\n\u001b[1;32m----> 2\u001b[0m     returns: \u001b[43mUnion\u001b[49m[pd\u001b[38;5;241m.\u001b[39mDataFrame, List[pd\u001b[38;5;241m.\u001b[39mSeries]],\n\u001b[0;32m      3\u001b[0m     expected_returns: Union[pd\u001b[38;5;241m.\u001b[39mSeries, \u001b[38;5;28mdict\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m      4\u001b[0m     cov_matrix_factor: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m      5\u001b[0m     target_return: Union[\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m      6\u001b[0m     annual_factor: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m12\u001b[39m,\n\u001b[0;32m      7\u001b[0m     show_graphic: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m      8\u001b[0m     return_port_returns: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m      9\u001b[0m     name: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTangency\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     10\u001b[0m ):\n\u001b[0;32m     11\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124;03m    Calculates tangency portfolio weights based on the covariance matrix of returns.\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124;03m        When `target_return` is provided, the weights are rescaled to achieve the target return:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;124;03m    pd.DataFrame or pd.Series: Tangency portfolio weights or portfolio returns if `return_port_ret` is True.\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m     30\u001b[0m     returns \u001b[38;5;241m=\u001b[39m time_series_to_df(returns) \u001b[38;5;66;03m# Convert returns to DataFrame if it is a Series or a list of Series\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Union' is not defined"
     ]
    }
   ],
   "source": [
    "def calc_tangency_port(\n",
    "    returns: Union[pd.DataFrame, List[pd.Series]],\n",
    "    expected_returns: Union[pd.Series, dict, None] = None,\n",
    "    cov_matrix_factor: str = 1,\n",
    "    target_return: Union[None, float] = None,\n",
    "    annual_factor: int = 12,\n",
    "    show_graphic: bool = False,\n",
    "    return_port_returns: bool = False,\n",
    "    name: str = 'Tangency'\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculates tangency portfolio weights based on the covariance matrix of returns.\n",
    "        When `target_return` is provided, the weights are rescaled to achieve the target return:\n",
    "            - If returns are the \"excess returns\", then the rescaled tangency portfolio is also in the ~MV frontier.\n",
    "\n",
    "    Parameters:\n",
    "    returns (pd.DataFrame or List of pd.Series): Time series of returns.\n",
    "    expected_returns (pd.Series, dict or None, default=None): Expected returns for each asset. If None, uses the mean returns as a proxy for expected returns.\n",
    "    cov_matrix_factor (str, default=1): Weight for the covariance matrix. If 1, uses the sample covariance matrix, otherwise uses a shrinkage estimator.\n",
    "    target_return (float or None, default=None): Target return for rescaling weights (annualized).\n",
    "    annual_factor (int, default=12): Factor for annualizing returns.\n",
    "    show_graphic (bool, default=False): If True, plots the tangency weights.\n",
    "    return_port_returns (bool, default=False): If True, returns the portfolio returns. Otherwise, returns portfolio weights.\n",
    "    name (str, default='Tangency'): Name for labeling the weights and portfolio.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame or pd.Series: Tangency portfolio weights or portfolio returns if `return_port_ret` is True.\n",
    "    \"\"\"\n",
    "\n",
    "    returns = time_series_to_df(returns) # Convert returns to DataFrame if it is a Series or a list of Series\n",
    "    fix_dates_index(returns) # Fix the date index of the DataFrame if it is not in datetime format and convert returns to float\n",
    "\n",
    "    # Calculate the covariance matrix\n",
    "    if cov_matrix_factor == 1:\n",
    "        cov_matrix = returns.cov()\n",
    "    else:\n",
    "        cov_matrix = returns.cov()\n",
    "        cov_matrix_diag = np.diag(np.diag(cov_matrix))\n",
    "        cov_matrix = cov_matrix_factor * cov_matrix + (1-cov_matrix_factor) * cov_matrix_diag\n",
    "    \n",
    "    cov_matrix_inv = np.linalg.pinv(cov_matrix)\n",
    "    ones = np.ones(len(returns.columns))\n",
    "    if expected_returns is not None:\n",
    "        if isinstance(expected_returns, dict):\n",
    "            expected_returns = pd.Series(expected_returns)\n",
    "        elif isinstance(expected_returns, pd.DataFrame):\n",
    "            expected_returns = expected_returns.iloc[:, 0]\n",
    "        else:\n",
    "            raise TypeError('expected_returns must be a pd.Series or a dictionary')\n",
    "        \n",
    "        mu = expected_returns.reindex(returns.columns)\n",
    "        if mu.isnull().any():\n",
    "            not_in_returns_mu = mu[mu.isnull()].index\n",
    "            raise Exception(f'{not_in_returns_mu} not in returns columns')\n",
    "    else:\n",
    "        mu = returns.mean() # Use mean monthly excess returns as a proxy for expected excess returns: (mu)\n",
    "\n",
    "    # Calculate the tangency portfolio weights\n",
    "    scaling = 1 / (ones.T @ cov_matrix_inv @ mu)\n",
    "    tangency_wts = scaling * (cov_matrix_inv @ mu)\n",
    "    tangency_wts = pd.DataFrame(index=returns.columns, data=tangency_wts, columns=[f'{name} Portfolio'])\n",
    "    \n",
    "    # Calculate the portfolio returns\n",
    "    port_returns = returns @ tangency_wts\n",
    "\n",
    "    # Rescale weights to target return\n",
    "    if isinstance(target_return, (float, int)):\n",
    "        if annual_factor is None:\n",
    "            print(f'Assuming monthly returns with annualization term of 12 since none was provided')\n",
    "            annual_factor = 12\n",
    "        scaler = target_return / (port_returns[f'{name} Portfolio'].mean() * annual_factor)\n",
    "        tangency_wts[[f'{name} Portfolio']] *= scaler\n",
    "        port_returns *= scaler\n",
    "        \n",
    "        tangency_wts = tangency_wts.rename({f'{name} Portfolio': f'{name} Portfolio (rescaled {target_return:.1%} p.a.)'},axis=1)\n",
    "        port_returns = port_returns.rename({f'{name} Portfolio': f'{name} Portfolio (rescaled {target_return:.1%} p.a.)'},axis=1)\n",
    "\n",
    "    \n",
    "    # Plot the tangency weights\n",
    "    if show_graphic == True:\n",
    "        ax = tangency_wts.plot(kind='bar', title=f'{name} Portfolio Weights')\n",
    "        for p in ax.patches:\n",
    "            ax.annotate(f'{p.get_height():.2%}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                ha='center', va='center', xytext=(0, 10), textcoords='offset points')\n",
    "\n",
    "    if cov_matrix_factor != 1:\n",
    "        if target_return is None:\n",
    "            tangency_wts = tangency_wts.rename({f'{name} Portfolio': f'{name} Portfolio (regularized {cov_matrix_factor:.1f})'},axis=1)\n",
    "            port_returns = port_returns.rename({f'{name} Portfolio':f'{name} Portfolio (regularized {cov_matrix_factor:.1f})'},axis=1)\n",
    "        else:\n",
    "            tangency_wts = tangency_wts.rename({f'{name} Portfolio (rescaled {target_return:.1%} p.a.)':\n",
    "                                                f'{name} Portfolio (regularized {cov_matrix_factor:.1f}, rescaled {target_return:.1%} p.a.)'},axis=1)\n",
    "            port_returns = port_returns.rename({f'{name} Portfolio (rescaled {target_return:.1%} p.a.)':\n",
    "                                                f'{name} Portfolio (regularized {cov_matrix_factor:.1f}, rescaled {target_return:.1%} p.a.)'},axis=1)\n",
    "            \n",
    "        \n",
    "    if return_port_returns:\n",
    "        return port_returns\n",
    "    \n",
    "    return tangency_wts\n",
    "\n",
    "\n",
    "def calc_equal_weights_port(\n",
    "    returns: Union[pd.DataFrame, List[pd.Series]],\n",
    "    target_return: Union[float, None] = None,\n",
    "    annual_factor: int = 12,\n",
    "    show_graphic: bool = False,\n",
    "    return_port_returns: bool = False,\n",
    "    name: str = 'Equal Weights'\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculates equal weights for the portfolio based on the provided returns.\n",
    "\n",
    "    Parameters:\n",
    "    returns (pd.DataFrame or List or pd.Series): Time series of returns.\n",
    "    target_return (float or None, default=None): Target return for rescaling weights (annualized).\n",
    "    annual_factor (int, default=12): Factor for annualizing returns.\n",
    "    show_graphic (bool, default=False): If True, plots the equal weights.\n",
    "    return_port_returns (bool, default=False): If True, returns the portfolio returns. Otherwise, returns portfolio weights.\n",
    "    name (str, default='Equal Weights'): Name for labeling the portfolio.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame or pd.Series: Equal portfolio weights or portfolio returns if `return_port_returns` is True.\n",
    "    \"\"\"\n",
    "\n",
    "    returns = time_series_to_df(returns) # Convert returns to DataFrame if it is a Series or a list of Series\n",
    "    fix_dates_index(returns) # Fix the date index of the DataFrame if it is not in datetime format and convert returns to float\n",
    "\n",
    "    equal_wts = pd.DataFrame(\n",
    "        index=returns.columns,\n",
    "        data=[1 / len(returns.columns)] * len(returns.columns),\n",
    "        columns=[f'{name} Portfolio']\n",
    "    )\n",
    "    port_returns = returns @ equal_wts\n",
    "    if isinstance(target_return, (float, int)):\n",
    "        if annual_factor is None:\n",
    "            print(f'Assuming monthly returns with annualization term of 12 since none was provided')\n",
    "            annual_factor = 12\n",
    "        scaler = target_return / (port_returns[f'{name} Portfolio'].mean() * annual_factor)\n",
    "        equal_wts[[f'{name} Portfolio']] *= scaler\n",
    "        port_returns *= scaler\n",
    "        \n",
    "        equal_wts = equal_wts.rename(\n",
    "            {f'{name} Portfolio': f'{name} Portfolio (rescaled {target_return:.1%} p.a.)'},axis=1)\n",
    "        port_returns = port_returns.rename(\n",
    "            {f'{name} Portfolio': f'{name} Portfolio (rescaled {target_return:.1%} p.a.)'},axis=1)\n",
    "\n",
    "    # Plot the equal weights\n",
    "    if show_graphic:\n",
    "        ax = equal_wts.plot(kind='bar', title=f'{name} Portfolio Weights')\n",
    "        for p in ax.patches:\n",
    "            ax.annotate(f'{p.get_height():.2%}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                ha='center', va='center', xytext=(0, 10), textcoords='offset points')\n",
    "        \n",
    "    if return_port_returns:\n",
    "        return port_returns\n",
    "    return equal_wts\n",
    "\n",
    "\n",
    "def calc_risk_parity_port(\n",
    "    returns: Union[pd.DataFrame, List[pd.Series]],\n",
    "    optimized: bool = False,\n",
    "    target_return: Union[None, float] = None,\n",
    "    annual_factor: int = 12,\n",
    "    show_graphic: bool = False,\n",
    "    return_port_returns: bool = False,\n",
    "    name: str = 'Risk Parity'\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculates risk parity portfolio weights based on the variance of each asset.\n",
    "\n",
    "    Parameters:\n",
    "    returns (pd.DataFrame or List or pd.Series): Time series of returns.\n",
    "    optimized (bool, default=False): If True, uses an optimization algorithm to calculate the risk parity weights.\n",
    "    target_return (float or None, default=None): Target return for rescaling weights (annualized).\n",
    "    annual_factor (int, default=12): Factor for annualizing returns.\n",
    "    show_graphic (bool, default=False): If True, plots the risk parity weights.\n",
    "    return_port_returns (bool, default=False): If True, returns the portfolio returns. Otherwise, returns portfolio weights.\n",
    "    name (str, default='Risk Parity'): Name for labeling the portfolio.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame or pd.Series: Risk parity portfolio weights or portfolio returns if `return_port_ret` is True.\n",
    "    \"\"\"\n",
    "\n",
    "    # Objective function for risk parity optimization\n",
    "    #  - Calculate individual asset risk contributions\n",
    "    #  - The objective is to minimize the squared differences in risk contributions\n",
    "    def objective_function_RP(weights, cov_matrix):    \n",
    "        marginal_contributions = cov_matrix @ weights\n",
    "        risk_contributions = weights * marginal_contributions\n",
    "        target_risk = np.mean(risk_contributions)\n",
    "        return np.sum((risk_contributions - target_risk) ** 2)\n",
    "\n",
    "    returns = time_series_to_df(returns) # Convert returns to DataFrame if it is a Series or a list of Series\n",
    "    fix_dates_index(returns) # Fix the date index of the DataFrame if it is not in datetime format and convert returns to float\n",
    "\n",
    "    # Calculaye weights for risk parity\n",
    "    weights = [1 / returns[asset].var() for asset in returns.columns] # Inverse of the variance (simple approach)\n",
    "    if optimized: # Optimized approach\n",
    "        cov_matrix = returns.cov()\n",
    "        weights = minimize(objective_function_RP,\n",
    "                        x0=weights,  # Initial guess (equal weights)\n",
    "                        args=(cov_matrix,),  # Additional arguments passed to the objective function\n",
    "                        bounds=None,  # No bounds, allowing for leverage\n",
    "                        constraints=None,  # No constraints, allowing for leverage\n",
    "                        tol=1e-13  # Precision tolerance\n",
    "                        ).x\n",
    "        \n",
    "    risk_parity_wts = pd.DataFrame(\n",
    "        index=returns.columns,\n",
    "        data=weights,\n",
    "        columns=[f'{name} Portfolio']\n",
    "    )\n",
    "\n",
    "    port_returns = returns @ risk_parity_wts\n",
    "\n",
    "    if isinstance(target_return, (float, int)):\n",
    "        if annual_factor is None:\n",
    "            print(f'Assuming monthly returns with annualization term of 12 since none was provided')\n",
    "            annual_factor = 12\n",
    "        scaler = target_return / (port_returns[f'{name} Portfolio'].mean() * annual_factor)\n",
    "        risk_parity_wts[[f'{name} Portfolio']] *= scaler\n",
    "        port_returns *= scaler\n",
    "\n",
    "        risk_parity_wts = risk_parity_wts.rename(\n",
    "            {f'{name} Portfolio': f'{name} Portfolio (rescaled {target_return:.1%} p.a.)'},axis=1)\n",
    "        port_returns = port_returns.rename(\n",
    "            {f'{name} Portfolio': f'{name} Portfolio (rescaled {target_return:.1%} p.a.)'},axis=1)\n",
    "        \n",
    "    if optimized == True:\n",
    "        if target_return is None:\n",
    "            port_returns = port_returns.rename({f'{name} Portfolio': f'{name} Portfolio (optimized)'}, axis = 1)\n",
    "            risk_parity_wts = risk_parity_wts.rename({f'{name} Portfolio': f'{name} Portfolio (optimized)'}, axis = 1)\n",
    "        else:\n",
    "            port_returns = port_returns.rename({f'{name} Portfolio (rescaled {target_return:.1%} p.a.)':\n",
    "                                                f'{name} Portfolio (optimized, rescaled {target_return:.1%} p.a.)'}, axis = 1)\n",
    "            \n",
    "            risk_parity_wts = risk_parity_wts.rename({f'{name} Portfolio (rescaled {target_return:.1%} p.a.)':\n",
    "                                                    f'{name} Portfolio (optimized, rescaled {target_return:.1%} p.a.)'}, axis = 1)\n",
    "\n",
    "    # Plot the risk parity weights\n",
    "    if show_graphic:\n",
    "        ax = risk_parity_wts.plot(kind='bar', title=f'{name} Portfolio Weights')\n",
    "        for p in ax.patches:\n",
    "            ax.annotate(f'{p.get_height():.2%}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                ha='center', va='center', xytext=(0, 10), textcoords='offset points')\n",
    "        \n",
    "    if return_port_returns:\n",
    "        return port_returns\n",
    "    return risk_parity_wts\n",
    "\n",
    "\n",
    "def calc_gmv_port(\n",
    "    returns: Union[pd.DataFrame, List[pd.Series]],\n",
    "    cov_matrix_factor: str = 1,\n",
    "    target_return: Union[float, None] = None,\n",
    "    annual_factor: int = 12,\n",
    "    show_graphic: bool = False,\n",
    "    return_port_returns: bool = False,\n",
    "    name: str = 'GMV'\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculates Global Minimum Variance (GMV) portfolio weights.\n",
    "\n",
    "    Parameters:\n",
    "    returns (pd.DataFrame): Time series of returns.\n",
    "    cov_matrix_factor (str, default=1): Weight for the covariance matrix. If 1, uses the sample covariance matrix, otherwise uses a shrinkage estimator.\n",
    "    target_return (float or None, default=None): Target return for rescaling weights (annualized).\n",
    "    annual_factor (int, default=12): Factor for annualizing returns.\n",
    "    show_graphic (bool, default=False): If True, plots the GMV weights.\n",
    "    return_port_returns (bool, default=False): If True, returns the portfolio returns. Otherwise, returns portfolio weights.\n",
    "    name (str, default='GMV'): Name for labeling the portfolio.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame or pd.Series: GMV portfolio weights or portfolio returns if `return_port_ret` is True.\n",
    "    \"\"\"\n",
    "    returns = time_series_to_df(returns) # Convert returns to DataFrame if it is a Series or a list of Series\n",
    "    fix_dates_index(returns) # Fix the date index of the DataFrame if it is not in datetime format and convert returns to float\n",
    "\n",
    "    # Calculate the covariance matrix\n",
    "    if cov_matrix_factor == 1:\n",
    "        cov_matrix = returns.cov()\n",
    "    else:\n",
    "        cov_matrix = returns.cov()\n",
    "        cov_matrix_diag = np.diag(np.diag(cov_matrix))\n",
    "        cov_matrix = cov_matrix_factor * cov_matrix + (1-cov_matrix_factor) * cov_matrix_diag\n",
    "    \n",
    "    cov_matrix_inv = np.linalg.pinv(cov_matrix)\n",
    "    ones = np.ones(len(returns.columns))\n",
    "\n",
    "    # Calculate the GMV portfolio weights\n",
    "    scaling = 1 / (ones.T @ cov_matrix_inv @ ones)\n",
    "    gmv_wts = scaling * cov_matrix_inv @ ones\n",
    "    gmv_wts = pd.DataFrame(index=returns.columns, data=gmv_wts, columns=[f'{name} Portfolio'])\n",
    "    \n",
    "    # Calculate the portfolio returns\n",
    "    port_returns = returns @ gmv_wts\n",
    "\n",
    "    # Rescale weights to target return\n",
    "    if isinstance(target_return, (float, int)):\n",
    "        if annual_factor is None:\n",
    "            print(f'Assuming monthly returns with annualization term of 12 since none was provided')\n",
    "            annual_factor = 12\n",
    "        scaler = target_return / (port_returns[f'{name}'].mean() * annual_factor)\n",
    "        gmv_wts[[f'{name} Portfolio']] *= scaler\n",
    "        port_returns *= scaler\n",
    "\n",
    "        gmv_wts = gmv_wts.rename({f'{name} Portfolio': f'{name} Portfolio (rescaled {target_return:.1%} p.a.)'},axis=1)\n",
    "        port_returns = port_returns.rename({f'{name} Portfolio': f'{name} Portfolio (rescaled {target_return:.1%} p.a.)'},axis=1)\n",
    "        \n",
    "\n",
    "    # Plot the Global Minimum Variance weights\n",
    "    if show_graphic:\n",
    "        ax = gmv_wts.plot(kind='bar', title=f'{name} Portfolio Weights')\n",
    "        for p in ax.patches:\n",
    "            ax.annotate(f'{p.get_height():.2%}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                ha='center', va='center', xytext=(0, 10), textcoords='offset points')\n",
    "\n",
    "    if cov_matrix_factor != 1:\n",
    "        if target_return is None:\n",
    "            gmv_wts = gmv_wts.rename({f'{name} Portfolio': f'{name} Portfolio (regularized {cov_matrix_factor:.1f})'},axis=1)\n",
    "            port_returns = port_returns.rename({f'{name} Portfolio':f'{name} Portfolio (regularized {cov_matrix_factor:.1f})'},axis=1)\n",
    "        else:\n",
    "            gmv_wts = gmv_wts.rename({f'{name} Portfolio (rescaled {target_return:.1%} p.a.)':\n",
    "                                                f'{name} Portfolio (regularized {cov_matrix_factor:.1f}, rescaled {target_return:.1%} p.a.)'},axis=1)\n",
    "            port_returns = port_returns.rename({f'{name} Portfolio (rescaled {target_return:.1%} p.a.)':\n",
    "                                                f'{name} Portfolio (regularized {cov_matrix_factor:.1f}, rescaled {target_return:.1%} p.a.)'},axis=1)\n",
    "        \n",
    "    if return_port_returns:\n",
    "        return port_returns\n",
    "\n",
    "    return gmv_wts\n",
    "\n",
    "\n",
    "def calc_mv_port(\n",
    "    returns: Union[pd.DataFrame, List[pd.Series]],\n",
    "    target_return: float = None,\n",
    "    is_excess_returns: bool = None,\n",
    "    annual_factor: int = 12,\n",
    "    show_graphic: bool = False,\n",
    "    return_port_ret: bool = False,\n",
    "    name: str = 'MV'\n",
    "):\n",
    "    \n",
    "    \"\"\"\n",
    "    Calculates the portfolio weights to achieve a target return by combining Tangency and GMV portfolios.\n",
    "\n",
    "    Parameters:\n",
    "    returns (pd.DataFrame): Time series of asset returns.\n",
    "    is_excess_returns (bool, default=False): if True, then assume risk free is available and the MV portfolio with target return is the rescaled tangency portfollio\n",
    "                                             if False, then assume risk free is not available and the MV portfolio with target return is a combination of the tangency and GMV portfolios\n",
    "    target_return (float): Target return for the portfolio.\n",
    "    annual_factor (int, default=12): Factor for annualizing\n",
    "    show_graphic (bool, default=False): If True, plots the portfolio weights.\n",
    "    return_port_returns (bool, default=False): If True, returns the portfolio returns. Otherwise, returns portfolio weights.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Weights of the Tangency and GMV portfolios, along with the combined target return portfolio.\n",
    "    \"\"\"\n",
    "\n",
    "    returns = time_series_to_df(returns) # Convert returns to DataFrame if it is a Series or a list of Series\n",
    "    fix_dates_index(returns) # Fix the date index of the DataFrame if it is not in datetime format and convert returns to float\n",
    "    \n",
    "    if not isinstance(target_return, (float, int)):\n",
    "        raise ValueError('target_return must be a float or an integer')\n",
    "    \n",
    "    if is_excess_returns is None:\n",
    "        raise ValueError('is_excess_returns must be a boolean')\n",
    "    \n",
    "    if annual_factor is None:\n",
    "            print(f'Assuming monthly returns with annualization term of 12 since none was provided')\n",
    "            annual_factor = 12\n",
    "    \n",
    "    elif is_excess_returns == True:\n",
    "        mv_portfolio = calc_tangency_port(\n",
    "            returns = returns,\n",
    "            target_return = target_return,\n",
    "            annual_factor = annual_factor,\n",
    "            show_graphic = show_graphic,\n",
    "            return_port_returns = return_port_ret\n",
    "        )\n",
    "        mv_portfolio.columns = [f'{name} Portfolio (target {target_return:.1%})']\n",
    "        return mv_portfolio\n",
    "    \n",
    "    else:\n",
    "        tan_weights = calc_tangency_port(returns, cov_matrix_factor=1)\n",
    "        gmv_weights = calc_gmv_port(returns)\n",
    "\n",
    "        mu_tan = returns.mean() @ tan_weights\n",
    "        mu_gmv = returns.mean() @ gmv_weights\n",
    "        \n",
    "        delta = (target_return - mu_gmv[0]) / (mu_tan[0] - mu_gmv[0])\n",
    "        mv_weights = (delta * calc_tangency_port(returns, cov_matrix_factor=1)).values + ((1 - delta) * calc_gmv_port(returns)).values\n",
    "        mv_weights = pd.DataFrame(\n",
    "            index=returns.columns,\n",
    "            data=mv_weights,\n",
    "            columns=[f'{name} Portfolio (target {target_return:.1%})']\n",
    "        )\n",
    "\n",
    "        port_returns = returns @ mv_weights\n",
    "        \n",
    "        if show_graphic:\n",
    "            mv_weights.plot(kind='bar', title=f'{name} Portfolio (target {target_return:.1%}) Weights')\n",
    "\n",
    "        if return_port_ret:\n",
    "            return port_returns\n",
    "        \n",
    "        #mv_weights['Tangency Portfolio'] = gmv_weights.values\n",
    "        #mv_weights['GMV Portfolio'] = gmv_weights.values\n",
    "        return mv_weights\n",
    "\n",
    "\n",
    "def calc_const_port_returns(\n",
    "    returns: Union[pd.DataFrame, List[pd.Series]],\n",
    "    weights: Union[dict, list, pd.Series, pd.DataFrame],\n",
    "    port_name: Union[None, str] = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates a portfolio by applying the specified constant weights to the asset returns.\n",
    "\n",
    "    Parameters:\n",
    "    returns (pd.DataFrame or List of pd.Series): Time series of asset returns.\n",
    "    weights (list or pd.Series): Weights to apply to the returns. If a list or pd.Series is provided, it will be converted into a dict.\n",
    "    port_name (str or None, default=None): Name for the portfolio. If None, a name will be generated based on asset weights.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The portfolio returns based on the provided weights.\n",
    "    \"\"\"\n",
    "\n",
    "    returns = time_series_to_df(returns) # Convert returns to DataFrame if it is a Series or a list of Series\n",
    "    fix_dates_index(returns) # Fix the date index of the DataFrame if it is not in datetime format and convert returns to float\n",
    "    \n",
    "    if isinstance(weights, list):\n",
    "        print(\"Weights are a list. Converting to dict assuming same order as columns in returns\")\n",
    "        weights = dict(zip(returns.columns, weights))\n",
    "    elif isinstance(weights, pd.Series):\n",
    "        weights = weights.to_dict()\n",
    "    elif isinstance(weights, pd.DataFrame):\n",
    "        weights = list(weights.to_dict().values())[0]\n",
    "    elif isinstance(weights, dict):\n",
    "        pass\n",
    "    else:\n",
    "        raise Exception(\"Weights must be a dict, list, pd.Series, or pd.DataFrame\")\n",
    "    \n",
    "    # Check returns size and weight size:\n",
    "    if returns.shape[1] != len(weights):\n",
    "        raise Exception(f\"Returns have {returns.shape[1]} assets, but {len(weights)} weights were provided\")\n",
    "\n",
    "    # Ensure columns match weights keys\n",
    "    returns = returns[list(weights.keys())]\n",
    "    port_returns = pd.DataFrame(returns @ list(weights.values()))\n",
    "\n",
    "    if port_name is None:\n",
    "        print(\"Portfolio: \"+\" + \".join([f\"{n} ({w:.2%})\" for n, w in weights.items()]))\n",
    "        port_name = 'Portfolio'\n",
    "    port_returns.columns = [port_name]\n",
    "\n",
    "    return port_returns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regressions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_regression(\n",
    "    Y: Union[pd.DataFrame, pd.Series, List[pd.Series]],\n",
    "    X: Union[pd.DataFrame, pd.Series, List[pd.Series]],\n",
    "    intercept: bool = True,\n",
    "    annual_factor: Union[None, int] = None,\n",
    "    return_model: bool = False,\n",
    "    return_fitted_values: bool = False,\n",
    "    p_values: bool = True,\n",
    "    tracking_error: bool = True,\n",
    "    r_squared: bool = True,\n",
    "    rse_mae: bool = True,\n",
    "    treynor_ratio: bool = False,\n",
    "    information_ratio: bool = False,\n",
    "    market_name: str = 'SPY US Equity',\n",
    "    sortino_ratio: bool = False,\n",
    "    timeframes: Union[None, dict] = None,\n",
    "    keep_columns: Union[list, str] = None,\n",
    "    drop_columns: Union[list, str] = None,\n",
    "    keep_indexes: Union[list, str] = None,\n",
    "    drop_indexes: Union[list, str] = None\n",
    "    ):\n",
    "    \n",
    "    \"\"\"\n",
    "    Performs an OLS regression of a \"many-to-many\" returns time series with optional intercept, timeframes, statistical ratios, and performance ratios.\n",
    "\n",
    "    Parameters:\n",
    "    y (pd.DataFrame, pd.Series or List or pd.Series): Dependent variable(s) for the regression.\n",
    "    X (pd.DataFrame, pd.Series or List or pd.Series): Independent variable(s) for the regression.\n",
    "    intercept (bool, default=True): If True, includes an intercept in the regression.\n",
    "    annual_factor (int or None, default=None): Factor for annualizing regression statistics.\n",
    "    return_model (bool, default=False): If True, returns the regression model object.\n",
    "    return_fitted_values (bool, default=False): If True, returns the fitted values of the regression.\n",
    "    p_values (bool, default=True): If True, displays p-values for the regression coefficients.\n",
    "    tracking_error (bool, default=True): If True, calculates the tracking error of the regression.\n",
    "    r_squared (bool, default=True): If True, calculates the R-squared of the regression.\n",
    "    rse_mae (bool, default=False): If True, calculates the Mean Absolute Error (MAE) and Relative Squared Error (RSE) of the regression.\n",
    "    treynor_ratio (bool, default=False): If True, calculates Treynor ratio.\n",
    "    information_ratio (bool, default=False): If True, calculates Information ratio.\n",
    "    market_name (str, default='SPY US Equity'): Name of the market index for the Treynor ratio.\n",
    "    sortino_ratio (bool, default=False): If True, calculates the Sortino ratio.\n",
    "    timeframes (dict or None, default=None): Dictionary of timeframes to run separate regressions for each period.\n",
    "    keep_columns (list or str, default=None): Columns to keep in the resulting DataFrame.\n",
    "    drop_columns (list or str, default=None): Columns to drop from the resulting DataFrame.\n",
    "    keep_indexes (list or str, default=None): Indexes to keep in the resulting DataFrame.\n",
    "    drop_indexes (list or str, default=None): Indexes to drop from the resulting DataFrame.\n",
    "    calc_sortino_ratio (bool, default=False): If True, calculates the Sortino ratio.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame or model: Regression summary statistics or the model if `return_model` is True.\n",
    "    \"\"\"\n",
    "\n",
    "    X = time_series_to_df(X) # Convert returns to DataFrame if it is a Series or a list of Series\n",
    "    fix_dates_index(X) # Fix the date index of the DataFrame if it is not in datetime format and convert returns to float\n",
    "\n",
    "    Y = time_series_to_df(Y) # Convert returns to DataFrame if it is a Series or a list of Series\n",
    "    fix_dates_index(Y) # Fix the date index of the DataFrame if it is not in datetime format and convert returns to float\n",
    "\n",
    "    if annual_factor is None:\n",
    "        print(\"Regression assumes 'annual_factor' equals to 12 since it was not provided\")\n",
    "        annual_factor = 12\n",
    "    \n",
    "    y_names = list(Y.columns) if isinstance(Y, pd.DataFrame) else [Y.name]\n",
    "    X_names = \" + \".join(list(X.columns))\n",
    "    X_names = \"Intercept + \" + X_names if intercept else X_names\n",
    "\n",
    "    # Add the intercept\n",
    "    if intercept:\n",
    "        X = sm.add_constant(X)\n",
    " \n",
    "    # Check if y and X have the same length\n",
    "    if len(X.index) != len(Y.index):\n",
    "        print(f'y has lenght {len(Y.index)} and X has lenght {len(X.index)}. Joining y and X by y.index...')\n",
    "        df = Y.join(X, how='left')\n",
    "        df = df.dropna()\n",
    "        Y = df[y_names]\n",
    "        X = df.drop(columns=y_names)\n",
    "        if len(X.index) < len(X.columns) + 1:\n",
    "            raise Exception('Indexes of y and X do not match and there are less observations than degrees of freedom. Cannot calculate regression')\n",
    "\n",
    "\n",
    "    if isinstance(timeframes, dict):\n",
    "        all_timeframes_regressions = pd.DataFrame()\n",
    "        for name, timeframe in timeframes.items():\n",
    "            if timeframe[0] and timeframe[1]:\n",
    "                timeframe_Y = Y.loc[timeframe[0]:timeframe[1]]\n",
    "                timeframe_X = X.loc[timeframe[0]:timeframe[1]]\n",
    "            elif timeframe[0]:\n",
    "                timeframe_Y = Y.loc[timeframe[0]:]\n",
    "                timeframe_X = X.loc[timeframe[0]:]\n",
    "            elif timeframe[1]:\n",
    "                timeframe_Y = Y.loc[:timeframe[1]]\n",
    "                timeframe_X = X.loc[:timeframe[1]]\n",
    "            else:\n",
    "                timeframe_Y = Y.copy()\n",
    "                timeframe_X = X.copy()\n",
    "            if len(timeframe_Y.index) == 0 or len(timeframe_X.index) == 0:\n",
    "                raise Exception(f'No returns data for {name} timeframe')\n",
    "            \n",
    "            timeframe_Y = timeframe_Y.rename(columns=lambda col: col + f' ({name})')\n",
    "            timeframe_regression = calc_regression(\n",
    "                Y=timeframe_Y,\n",
    "                X=timeframe_X,\n",
    "                intercept=intercept,\n",
    "                annual_factor=annual_factor,\n",
    "                warnings=False,\n",
    "                return_model=False,\n",
    "                return_fitted_values=False,\n",
    "                p_values=p_values,\n",
    "                tracking_error=tracking_error,\n",
    "                r_squared=r_squared,\n",
    "                rse_mae=rse_mae,\n",
    "                treynor_ratio=treynor_ratio,\n",
    "                information_ratio=information_ratio,\n",
    "                timeframes=None,\n",
    "                keep_columns=keep_columns,\n",
    "                drop_columns=drop_columns,\n",
    "                keep_indexes=keep_indexes,\n",
    "                drop_indexes=drop_indexes\n",
    "            )\n",
    "            timeframe_regression.index = [f\"{timeframe_regression.index} ({name})\"]\n",
    "            all_timeframes_regressions = pd.concat(\n",
    "                [all_timeframes_regressions, timeframe_regression],\n",
    "                axis=0\n",
    "            )\n",
    "        return all_timeframes_regressions\n",
    "    \n",
    "    regression_statistics = pd.DataFrame(index=y_names, columns=[])\t\n",
    "    fitted_values_all = pd.DataFrame(index=Y.index, columns=y_names)\n",
    "\n",
    "    for y_asset in y_names:\n",
    "        # Fit the regression model: \n",
    "        y = Y[y_asset]\n",
    "        try:\n",
    "            ols_model = sm.OLS(y, X, missing=\"drop\")\n",
    "        except ValueError:\n",
    "            y = y.reset_index(drop=True)\n",
    "            X = X.reset_index(drop=True)\n",
    "            ols_model = sm.OLS(y, X, missing=\"drop\")\n",
    "            print(f'\"{y_asset}\" Required to reset indexes to make regression work. Try passing \"y\" and \"X\" as pd.DataFrame')\n",
    "        \n",
    "        ols_results = ols_model.fit()\n",
    "\n",
    "        if return_model:\n",
    "            return(ols_results)\n",
    "\n",
    "        elif return_fitted_values:\n",
    "            fitted_values = ols_results.fittedvalues\n",
    "            fitted_values = fitted_values.rename(f'{y_asset}^')\n",
    "            fitted_values_all[y_asset] = fitted_values\n",
    "\n",
    "        else:\n",
    "            # Calculate/get statistics:\n",
    "\n",
    "            if r_squared == True:\n",
    "                regression_statistics.loc[y_asset, 'R-Squared'] = ols_results.rsquared # R-squared\n",
    "                if intercept == False:\n",
    "                    print('No intercept in regression. R-Squared might not make statistical sense.')\n",
    "\n",
    "            regression_statistics.loc[y_asset, 'Observed Mean'] = y.mean()\n",
    "            regression_statistics.loc[y_asset, 'Observed Std Dev'] = y.std()\n",
    "\n",
    "            # Residual Standard Error (RSE) and Mean Absolute Error (MAE)\n",
    "            residuals =  ols_results.resid\n",
    "            rse = (sum(residuals**2) / (len(residuals) - len(ols_results.params))) ** 0.5 \n",
    "            \n",
    "            if rse_mae:\n",
    "                regression_statistics.loc[y_asset, 'RSE'] = rse\n",
    "                regression_statistics.loc[y_asset, 'MAE'] = abs(residuals).mean()\n",
    "\n",
    "            if intercept == True:\n",
    "                regression_statistics.loc[y_asset, 'Alpha'] = ols_results.params.iloc[0]\n",
    "                regression_statistics.loc[y_asset, 'Annualized Alpha'] = ols_results.params.iloc[0] * annual_factor # Annualized Alpha \n",
    "                \n",
    "                if p_values == True: \n",
    "                    regression_statistics.loc[y_asset, 'P-Value (Alpha)'] = ols_results.pvalues.iloc[0] # Alpha p-value\n",
    "\n",
    "            # Process betas and p-values for explanatory variables\n",
    "            X_names = list(X.columns[1:]) if intercept else list(X.columns)\n",
    "            betas = ols_results.params[1:] if intercept else ols_results.params\n",
    "            betas_p_values = ols_results.pvalues[1:] if intercept else ols_results.pvalues\n",
    "            \n",
    "            for i in range(len(X_names)):\n",
    "                regression_statistics.loc[y_asset, f\"Beta ({X_names[i]})\"] = betas.iloc[i] # Betas\n",
    "                if p_values == True: \n",
    "                    regression_statistics.loc[y_asset, f\"P-Value ({X_names[i]})\"] = betas_p_values.iloc[i] # Beta p-values\n",
    "\n",
    "            if tracking_error == True:\n",
    "                regression_statistics.loc[y_asset, 'Tracking Error'] = residuals.std() \n",
    "                regression_statistics.loc[y_asset, 'Annualized Tracking Error'] = regression_statistics.loc[y_asset, 'Tracking Error'] * (annual_factor ** 0.5) # Annualized Residuals Volatility\n",
    "\n",
    "            if treynor_ratio == True:\n",
    "                market_names = ['SPY', 'SPX', 'SP500', 'SPY US Equity']\n",
    "                if market_name not in market_names:\n",
    "                    print(f'Neither {market_name} are a factor in the regression. Treynor Ratio cannot be calculated.')\n",
    "                else:\n",
    "                    market_name = [m for m in market_names if m in X.columns][0]\n",
    "                    try:\n",
    "                        regression_statistics.loc[y_asset, 'Treynor Ratio'] = y.mean() / regression_statistics.loc[y_asset, f'Beta ({market_name})'] # Treynor Ratio\n",
    "                        regression_statistics.loc[y_asset, 'Annualized Treynor Ratio'] = regression_statistics.loc[y_asset, 'Treynor Ratio'] * annual_factor # Annualized Treynor Ratio\n",
    "                    except:\n",
    "                        print(f'Treynor Ratio could not be calculated.')\n",
    "            if information_ratio == True:\n",
    "                if intercept:\n",
    "                    regression_statistics.loc[y_asset, 'Information Ratio'] = regression_statistics.loc[y_asset, 'Alpha'] / residuals.std() # Information Ratio\n",
    "                    regression_statistics.loc[y_asset, 'Annualized Information Ratio'] = regression_statistics.loc[y_asset, 'Information Ratio'] * (annual_factor ** 0.5) # Annualized Information Ratio\n",
    "            \n",
    "            if sortino_ratio:\n",
    "                try:\n",
    "                    regression_statistics.loc[y_asset, 'Sortino Ratio'] = regression_statistics.loc[y_asset, 'Fitted Mean'] / Y[Y < 0].std()\n",
    "                except Exception as e:\n",
    "                    print(f'Cannot calculate Sortino Ratio: {str(e)}. Set \"calc_sortino_ratio\" to False or review function')\n",
    "    \n",
    "    if return_fitted_values:\n",
    "        return fitted_values_all\n",
    "    \n",
    "    else:\n",
    "        if regression_statistics.shape[0] == 1:\n",
    "            regression_statistics = regression_statistics.T\n",
    "        return filter_columns_and_indexes(\n",
    "            regression_statistics,\n",
    "            keep_columns=keep_columns,\n",
    "            drop_columns=drop_columns,\n",
    "            keep_indexes=keep_indexes,\n",
    "            drop_indexes=drop_indexes\n",
    "        )\n",
    "\n",
    "def calc_regression_rolling(\n",
    "    Y: Union[pd.DataFrame, pd.Series, List[pd.Series]],\n",
    "    X: Union[pd.DataFrame, pd.Series, List[pd.Series]],\n",
    "    intercept: bool = True,\n",
    "    window_size: int = 121,\n",
    "    betas_only: bool = True,\n",
    "    annual_factor: Union[None, int] = None,\n",
    "    tracking_error: bool = None,\n",
    "    r_squared: bool = None,\n",
    "    rse_mae: bool = None,\n",
    "    keep_columns: Union[list, str] = None,\n",
    "    drop_columns: Union[list, str] = None,\n",
    "    keep_indexes: Union[list, str] = None,\n",
    "    drop_indexes: Union[list, str] = None\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Performs a multiple OLS regression of a \"many-to-many\" returns time series with optional intercept, timeframes, statistical ratios, and performance ratios on a rolling window.\n",
    "    This is the first stage of a Fama-MacBeth model, used to estimate the betas for every asset for every window.\n",
    "\n",
    "    Parameters:\n",
    "    Y (pd.DataFrame, pd.Series or List of pd.Series): Dependent variable(s) for the regression.\n",
    "    X (pd.DataFrame, pd.Series or List of pd.Series): Independent variable(s) for the regression.\n",
    "    intercept (bool, default=True): If True, includes an intercept in the regression.\n",
    "    window_size (int, default=121): Size of the rolling window for in-sample fitting.\n",
    "\n",
    "    annual_factor (int or None, default=None): Factor for annualizing regression statistics.\n",
    "    return_model (bool, default=False): If True, returns the regression model object.\n",
    "    tracking_error (bool, default=True): If True, calculates the tracking error of the regression.\n",
    "    r_squared (bool, default=True): If True, calculates the R-squared of the regression.\n",
    "    rse_mae (bool, default=False): If True, calculates the Mean Absolute Error (MAE) and Relative Squared Error (RSE) of the regression.\n",
    "    keep_columns (list or str, default=None): Columns to keep in the resulting DataFrame.\n",
    "    drop_columns (list or str, default=None): Columns to drop from the resulting DataFrame.\n",
    "    keep_indexes (list or str, default=None): Indexes to keep in the resulting DataFrame.\n",
    "    drop_indexes (list or str, default=None): Indexes to drop from the resulting DataFrame.\n",
    "\n",
    "    Returns: a dataframe with the regression statistics for each rolling window.\n",
    "    \"\"\"\n",
    "    \n",
    "    X = time_series_to_df(X) # Convert returns to DataFrame if it is a Series or a list of Series\n",
    "    fix_dates_index(X) # Fix the date index of the DataFrame if it is not in datetime format and convert returns to float\n",
    "\n",
    "    Y = time_series_to_df(Y) # Convert returns to DataFrame if it is a Series or a list of Series\n",
    "    fix_dates_index(Y) # Fix the date index of the DataFrame if it is not in datetime format and convert returns to float\n",
    "    \n",
    "    if betas_only == True:\n",
    "        if r_squared == True or rse_mae == True or tracking_error == True:\n",
    "            print('Only betas are being calculated. Set \"betas_only\" to False to calculate other statistics.')\n",
    "    else:\n",
    "        if annual_factor is None:\n",
    "            print(\"Regression assumes 'annual_factor' equals to 12 since it was not provided\")\n",
    "            annual_factor = 12\n",
    "        if r_squared == True and intercept == False:\n",
    "            print('No intercept in regression. R-Squared might not make statistical sense.')\n",
    "    \n",
    "\n",
    "    y_names = list(Y.columns) if isinstance(Y, pd.DataFrame) else [Y.name]\n",
    "    X_names = \" + \".join(list(X.columns))\n",
    "    X_names = \"Intercept + \" + X_names if intercept else X_names\n",
    "\n",
    "    # Add the intercept\n",
    "    if intercept:\n",
    "        X = sm.add_constant(X)\n",
    " \n",
    "    # Check if y and X have the same length\n",
    "    if len(X.index) != len(Y.index):\n",
    "        print(f'y has lenght {len(Y.index)} and X has lenght {len(X.index)}. Joining y and X by y.index...')\n",
    "        df = Y.join(X, how='left')\n",
    "        df = df.dropna()\n",
    "        Y = df[y_names]\n",
    "        X = df.drop(columns=y_names)\n",
    "        if len(X.index) < len(X.columns) + 1:\n",
    "            raise Exception('Indexes of y and X do not match and there are less observations than degrees of freedom. Cannot calculate regression')\n",
    "        \n",
    "    regression_statistics = {}\n",
    "\n",
    "    X_names = list(X.columns[1:]) if intercept else list(X.columns)\n",
    "\n",
    "    # Loop through the rolling window\n",
    "    for i in range(window_size, len(Y.index), 1):\n",
    "\n",
    "        regression_statistics_i = pd.DataFrame(index=y_names, columns=[])\t\n",
    "\n",
    "        Y_i = Y.iloc[i-window_size:i]\n",
    "        X_i = X.iloc[i-window_size:i]\n",
    "\n",
    "        for y_asset in y_names:\n",
    "            # Fit the regression model: \n",
    "            y_i = Y_i[y_asset]\n",
    "            try:\n",
    "                ols_model = sm.OLS(y_i, X_i, missing=\"drop\")\n",
    "            except ValueError:\n",
    "                y_i = y_i.reset_index(drop=True)\n",
    "                X_i = X_i.reset_index(drop=True)\n",
    "                ols_model = sm.OLS(y_i, X_i, missing=\"drop\")\n",
    "                print(f'\"{y_asset}\" Required to reset indexes to make regression work. Try passing \"y\" and \"X\" as pd.DataFrame')\n",
    "            \n",
    "            ols_results = ols_model.fit() \n",
    "\n",
    "            if betas_only == True:\n",
    "                # Process betas for explanatory variables\n",
    "                betas = ols_results.params[1:] if intercept else ols_results.params\n",
    "                for j in range(len(X_names)):\n",
    "                    regression_statistics_i.loc[y_asset, f\"{X_names[j]}\"] = betas.iloc[j] # Betas\n",
    "            else:\n",
    "                if r_squared == True:\n",
    "                    regression_statistics_i.loc[y_asset, 'R-Squared'] = ols_results.rsquared # R-squared\n",
    "\n",
    "                # Residual Standard Error (RSE) and Mean Absolute Error (MAE)\n",
    "                residuals =  ols_results.resid\n",
    "                rse = (sum(residuals**2) / (len(residuals) - len(ols_results.params))) ** 0.5 \n",
    "                \n",
    "                if rse_mae:\n",
    "                    regression_statistics_i.loc[y_asset, 'RSE'] = rse\n",
    "                    regression_statistics_i.loc[y_asset, 'MAE'] = abs(residuals).mean()\n",
    "\n",
    "                if intercept == True:\n",
    "                    regression_statistics_i.loc[y_asset, 'Alpha'] = ols_results.params.iloc[0]\n",
    "                    regression_statistics_i.loc[y_asset, 'Annualized Alpha'] = ols_results.params.iloc[0] * annual_factor # Annualized Alpha \n",
    "                    \n",
    "                # Process betas for explanatory variables\n",
    "                betas = ols_results.params[1:] if intercept else ols_results.params\n",
    "                \n",
    "                for j in range(len(X_names)):\n",
    "                    regression_statistics_i.loc[y_asset, f\"Beta ({X_names[j]})\"] = betas.iloc[j] # Betas\n",
    "\n",
    "                if tracking_error == True:\n",
    "                    regression_statistics_i.loc[y_asset, 'Tracking Error'] = residuals.std() \n",
    "                    regression_statistics_i.loc[y_asset, 'Annualized Tracking Error'] = regression_statistics_i.loc[y_asset, 'Tracking Error'] * (annual_factor ** 0.5) # Annualized Residuals Volatility\n",
    "\n",
    "                regression_statistics_i =  filter_columns_and_indexes(regression_statistics,\n",
    "                                                                    keep_columns=keep_columns,\n",
    "                                                                    drop_columns=drop_columns,\n",
    "                                                                    keep_indexes=keep_indexes,\n",
    "                                                                    drop_indexes=drop_indexes)\n",
    "            \n",
    "        regression_statistics[Y.index[i]] = regression_statistics_i\n",
    "\n",
    "    return regression_statistics\n",
    "\n",
    "def calc_cross_section_regression(\n",
    "    Y: Union[pd.DataFrame, pd.Series, List[pd.Series]],\n",
    "    X: Union[pd.DataFrame, pd.Series, List[pd.Series]],\n",
    "    intercept: bool = True,\n",
    "    return_model: bool = False,\n",
    "    return_fitted_values: bool = False,\n",
    "    p_values: bool = True,\n",
    "    r_squared: bool = True,\n",
    "    rse_mae: bool = True,\n",
    "    regression_name: str = None,\n",
    "    keep_columns: Union[list, str] = None,\n",
    "    drop_columns: Union[list, str] = None,\n",
    "    keep_indexes: Union[list, str] = None,\n",
    "    drop_indexes: Union[list, str] = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Performs an OLS regression for cross-sectional data with optional intercept and statistical ratios.\n",
    "\n",
    "    Parameters:\n",
    "    Y (pd.DataFrame, pd.Series or List of pd.Series): Dependent variable(s) for the regression.\n",
    "    X (pd.DataFrame, pd.Series or List of pd.Series): Independent variable(s) for the regression.\n",
    "    intercept (bool, default=True): If True, includes an intercept in the regression.\n",
    "    return_model (bool, default=False): If True, returns the regression model object.\n",
    "    return_fitted_values (bool, default=False): If True, returns the fitted values of the regression.\n",
    "    p_values (bool, default=True): If True, displays p-values for the regression coefficients.\n",
    "    r_squared (bool, default=True): If True, calculates the R-squared of the regression.\n",
    "    rse_mae (bool, default=False): If True, calculates the Mean Absolute Error (MAE) and Relative Squared Error (RSE) of the regression.\n",
    "    keep_columns (list or str, default=None): Columns to keep in the resulting DataFrame.\n",
    "    drop_columns (list or str, default=None): Columns to drop from the resulting DataFrame.\n",
    "    keep_indexes (list or str, default=None): Indexes to keep in the resulting DataFrame.\n",
    "    drop_indexes (list or str, default=None): Indexes to drop from the resulting DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame or model: Regression summary statistics or the model if `return_model` is True.\n",
    "    \"\"\"\n",
    "\n",
    "    X = time_series_to_df(X)  # Convert inputs to DataFrame if not already\n",
    "    Y = time_series_to_df(Y)\n",
    "\n",
    "    y_names = list(Y.columns) if isinstance(Y, pd.DataFrame) else [Y.name]\n",
    "    X_names = \" + \".join(list(X.columns))\n",
    "    X_names = \"Intercept + \" + X_names if intercept else X_names\n",
    "\n",
    "    # Add intercept if specified\n",
    "    if intercept:\n",
    "        X = sm.add_constant(X)\n",
    "\n",
    "    # Check alignment of Y and X\n",
    "    if len(X.index) != len(Y.index):\n",
    "        raise Exception(f'Y has length {len(Y.index)} and X has length {len(X.index)}. Y and X must have the same length.')\n",
    "    \n",
    "    regression_statistics = pd.DataFrame(index=y_names, columns=[])\n",
    "    fitted_values_all = pd.DataFrame(index=Y.index, columns=y_names)\n",
    "\n",
    "    for y_asset in y_names:\n",
    "        # Fit the regression model\n",
    "        y = Y[y_asset]\n",
    "        try:\n",
    "            ols_model = sm.OLS(y, X, missing=\"drop\")\n",
    "        except ValueError:\n",
    "            y = y.reset_index(drop=True)\n",
    "            X = X.reset_index(drop=True)\n",
    "            ols_model = sm.OLS(y, X, missing=\"drop\")\n",
    "            print(f'\"{y_asset}\" Required to reset indexes to make regression work. Pass \"y\" and \"X\" as pd.DataFrame')\n",
    "\n",
    "        ols_results = ols_model.fit()\n",
    "\n",
    "        if return_model:\n",
    "            return ols_results\n",
    "\n",
    "        elif return_fitted_values:\n",
    "            fitted_values = ols_results.fittedvalues.rename(f'{y_asset}^')\n",
    "            fitted_values_all[y_asset] = fitted_values\n",
    "\n",
    "        else:\n",
    "            # Calculate/get statistics\n",
    "\n",
    "            if r_squared:\n",
    "                regression_statistics.loc[y_asset, 'R-Squared'] = ols_results.rsquared\n",
    "                if not intercept:\n",
    "                    print('No intercept in regression. R-Squared might not be interpretable.')\n",
    "            \n",
    "            # Mean and Standard Deviation of y\n",
    "            regression_statistics.loc[y_asset, 'Observed Mean'] = y.mean()\n",
    "            regression_statistics.loc[y_asset, 'Observed Std Dev'] = y.std()\n",
    "\n",
    "            # Residual Standard Error (RSE) and Mean Absolute Error (MAE)\n",
    "            residuals = ols_results.resid\n",
    "            rse = (sum(residuals**2) / (len(residuals) - len(ols_results.params))) ** 0.5 \n",
    "\n",
    "            if rse_mae:\n",
    "                regression_statistics.loc[y_asset, 'MAE'] = abs(residuals).mean()\n",
    "                regression_statistics.loc[y_asset, 'RSE'] = rse\n",
    "\n",
    "            if intercept:\n",
    "                regression_statistics.loc[y_asset, 'Intercept'] = ols_results.params.iloc[0]\n",
    "                if p_values:\n",
    "                    regression_statistics.loc[y_asset, 'P-Value (Intercept)'] = ols_results.pvalues.iloc[0]\n",
    "\n",
    "            # Process lambda and p-values for explanatory variables\n",
    "            X_names = list(X.columns[1:]) if intercept else list(X.columns)\n",
    "            lambdas = ols_results.params[1:] if intercept else ols_results.params\n",
    "            lambdas_p_values = ols_results.pvalues[1:] if intercept else ols_results.pvalues\n",
    "\n",
    "            for i, x_name in enumerate(X_names):\n",
    "                regression_statistics.loc[y_asset, f\"Lambda ({x_name})\"] = lambdas.iloc[i]\n",
    "                if p_values:\n",
    "                    regression_statistics.loc[y_asset, f\"P-Value ({x_name})\"] = lambdas_p_values.iloc[i]\n",
    "\n",
    "    if return_fitted_values:\n",
    "        return fitted_values_all\n",
    "    \n",
    "    else:\n",
    "        regression_statistics_t = regression_statistics.T\n",
    "        if regression_statistics_t.columns[0] == 0:\n",
    "            if regression_name is None:\n",
    "                regression_name = 'Cross-Sectional'\n",
    "            regression_statistics_t.rename(columns={0: regression_name}, inplace=True)\n",
    "        return filter_columns_and_indexes(\n",
    "            regression_statistics_t,\n",
    "            keep_columns=keep_columns,\n",
    "            drop_columns=drop_columns,\n",
    "            keep_indexes=keep_indexes,\n",
    "            drop_indexes=drop_indexes\n",
    "        )\n",
    "\n",
    "def calc_cross_section_regression_rolling(\n",
    "    y: Union[pd.DataFrame, pd.Series],\n",
    "    X: Dict[datetime.datetime, Union[pd.DataFrame, pd.Series]],\n",
    "    intercept: bool = True,\n",
    "    annual_factor: Union[None, int] = None,\n",
    "    regression_name: str = 'Rolling Cross-Section'\n",
    "):\n",
    "    \"\"\"\n",
    "    Performs a rolling OLS regression many-to-one for cross-sectional data with optional intercept.\n",
    "    This is the second stage of a Fama-MacBeth model, used to estimate the lambda coefficients for every factor for every window.\n",
    "\n",
    "    Parameters:\n",
    "    Y (pd.DataFrame, pd.Series or List of pd.Series): Single ependent variable for the regression.\n",
    "    X (dict): Dictionary of independent variable(s) for the regression, with dates as keys and DataFrames or Series as values containing the betas for each date.\n",
    "    intercept (bool, default=True): If True, includes an intercept in the regression.\n",
    "    annual_factor (int or None, default=None): Factor for annualizing regression statistics.\n",
    "    return_model (bool, default=False): If True, returns the regression model object.\n",
    "    return_fitted_values (bool, default=False): If True, returns the fitted values of the regression.\n",
    "    p_values (bool, default=True): If True, displays p-values for the regression coefficients.\n",
    "    r_squared (bool, default=True): If True, calculates the R-squared of the regression.\n",
    "    rse_mae (bool, default=False): If True, calculates the Mean Absolute Error (MAE) and Relative Squared Error (RSE) of the regression.\n",
    "    keep_columns (list or str, default=None): Columns to keep in the resulting DataFrame.\n",
    "    drop_columns (list or str, default=None): Columns to drop from the resulting DataFrame.\n",
    "    keep_indexes (list or str, default=None): Indexes to keep in the resulting DataFrame.\n",
    "    drop_indexes (list or str, default=None): Indexes to drop from the resulting DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    (pd.DataFrame, pd.DataFrame): time series of lambda coefficients and time series of residuals.\n",
    "    \"\"\"\n",
    "\n",
    "    y = time_series_to_df(y)  # Convert inputs to DataFrame if not already\n",
    "    \n",
    "    if annual_factor is None:\n",
    "        print(\"Regression assumes 'annual_factor' equals to 252 since it was not provided\")\n",
    "        annual_factor = 252\n",
    "        \n",
    "    dates = list(X.keys())\n",
    "    factor_names =  X[next(iter(X))].columns\n",
    "    lambdas_series = pd.DataFrame(index=dates, columns=factor_names)\n",
    "    mean_absolute_error = pd.DataFrame(index=dates, columns=['MAE'])\n",
    "\n",
    "    if len(dates) <= 1:\n",
    "        raise Exception('At least 2 dates are required to calculate rolling cross-section regression')\n",
    "    \n",
    "    if intercept == True:\n",
    "        intercept_series = pd.DataFrame(index=dates, columns=['Intercept'])\n",
    "\n",
    "    regression_summary = pd.DataFrame(index=['Rolling Cross-Section'])\n",
    "\n",
    "    for date, x in X.items():\n",
    "        y_i = y.loc[date]\n",
    "        X_i = x\n",
    "        cross_section_stats_i = calc_cross_section_regression(\n",
    "            Y=y_i,\n",
    "            X=X_i,\n",
    "            intercept=intercept,\n",
    "            return_model=False,\n",
    "            return_fitted_values=False,\n",
    "            p_values=False,\n",
    "            r_squared=False,\n",
    "            rse_mae=False\n",
    "        )\n",
    "\n",
    "        for factor in factor_names:\n",
    "            lambdas_series.loc[date, factor] = cross_section_stats_i.loc[f'Lambda ({factor})', :].values[0]\n",
    "\n",
    "        if intercept == True:\n",
    "            intercept_series.loc[date, 'Intercept'] = cross_section_stats_i.loc['Intercept', :].values[0]\n",
    "\n",
    "        total_abs_residuals = 0\n",
    "        for asset in y.columns:\n",
    "            residual = (y_i[asset]\n",
    "                       - sum(X_i.loc[asset, factor] * lambdas_series.loc[date, factor] for factor in lambdas_series.columns)\n",
    "                       - (intercept_series.loc[date, 'Intercept'] if intercept else 0))\n",
    "            total_abs_residuals += abs(residual)\n",
    "        mean_absolute_error_i = total_abs_residuals / len(y.columns)\n",
    "        mean_absolute_error.loc[date, ] = mean_absolute_error_i\n",
    "\n",
    "    regression_summary.loc[regression_name, 'Intercept (mean)'] = intercept_series.mean().values[0]\n",
    "    regression_summary.loc[regression_name, 'Intercept (std dev)'] = intercept_series.std().values[0]\n",
    "    \n",
    "    for factor in lambdas_series.columns:\n",
    "        regression_summary.loc[regression_name, f'{factor} (mean)'] = lambdas_series[factor].mean()\n",
    "        regression_summary.loc[regression_name, f'{factor} (std dev)'] = lambdas_series[factor].std()\n",
    "\n",
    "    regression_summary.loc[regression_name, 'MAE (mean)'] = mean_absolute_error.mean().values[0]\n",
    "\n",
    "    # Compute statistics directly from lambdas_series and intercept_series\n",
    "    for factor in lambdas_series.columns:\n",
    "        lambdas = lambdas_series[factor].dropna()\n",
    "        mean_lambda = lambdas.mean()\n",
    "        T = len(lambdas)\n",
    "        se_lambda = np.sqrt(np.sum((lambdas - mean_lambda)**2) / (T * (T - 1)))\n",
    "\n",
    "        regression_summary.loc[regression_name, f'{factor} (Annualized Mean)'] = mean_lambda * annual_factor\n",
    "        regression_summary.loc[regression_name, f'{factor} (Annualized SE)'] = se_lambda * annual_factor if T > 1 else None\n",
    "\n",
    "    if intercept:\n",
    "        intercepts = intercept_series['Intercept'].dropna()\n",
    "        mean_intercept = intercepts.mean()\n",
    "        T = len(intercepts)\n",
    "        se_intercept = np.sqrt(np.sum((intercepts - mean_intercept)**2) / (T * (T - 1)))\n",
    "        \n",
    "        regression_summary.loc[regression_name, 'Intercept (Annualized Mean)'] = mean_intercept * annual_factor\n",
    "        regression_summary.loc[regression_name, 'Intercept (Annualized SE)'] = se_intercept * annual_factor\n",
    "\n",
    "    regression_summary.loc[regression_name, 'MAE (mean)'] = mean_absolute_error.mean().values[0]\n",
    "\n",
    "    return regression_summary.T\n",
    "\n",
    "\n",
    "def calc_replication_oos_perf(\n",
    "    y: Union[pd.Series, pd.DataFrame],\n",
    "    X: Union[pd.Series, pd.DataFrame],\n",
    "    intercept: bool = True,\n",
    "    window_size: Union[None, int] = None,\n",
    "    rolling: bool = True,\n",
    "    lag_periods: int = 1,\n",
    "    return_model_param: float = False,\n",
    "    annual_factor: Union[None, int] = None,\n",
    "    significance_level: float = 0.05,\n",
    "    keep_columns: Union[list, str] = None,\n",
    "    drop_columns: Union[list, str] = None,\n",
    "    keep_indexes: Union[list, str] = None,\n",
    "    drop_indexes: Union[list, str] = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Performs out-of-sample regression to replicate asset from other assets/factors\n",
    "        using returns time series with rolling windows (default) or expanding windows.\n",
    "\n",
    "    Parameters:\n",
    "    y (pd.Series or pd.DataFrame): Dependent variable (actual returns).\n",
    "    X (pd.Series or pd.DataFrame): Independent variable(s) (predictors).\n",
    "    intercept (bool, default=True): If True, includes an intercept in the regression.\n",
    "    window_size (int or None, default=60): Size of the rolling window for in-sample fitting or the minimum number of observations for expanding window.\n",
    "    rolling (bool, default=rolling): If False, uses an expanding window instead of rolling.\n",
    "    lag_periods (int, default=1): Number of lags to apply to the predictors.\n",
    "    return_model_param (float, default=False): If True, returns the regression model statistics instead of predictions.\n",
    "    annual_factor (int or None, default=None): Factor for annualizing regression statistics.\n",
    "    significance_level (float, default=0.05): Level of significance (alpha) for evaluating parameters significance.\n",
    "    keep_columns (list or str, default=None): Columns to keep in the resulting DataFrame.\n",
    "    drop_columns (list or str, default=None): Columns to drop from the resulting DataFrame.\n",
    "    keep_indexes (list or str, default=None): Indexes to keep in the resulting DataFrame.\n",
    "    drop_indexes (list or str, default=None): Indexes to drop from the resulting DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Predictions for the out-of-sample replication or the model parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    X = time_series_to_df(X) # Convert returns to DataFrame if it is a Series or a list of Series\n",
    "    fix_dates_index(X) # Fix the date index of the DataFrame if it is not in datetime format and convert returns to float\n",
    "\n",
    "    y = time_series_to_df(y) # Convert returns to DataFrame if it is a Series or a list of Series\n",
    "    fix_dates_index(y) # Fix the date index of the DataFrame if it is not in datetime format and convert returns to float\n",
    "\n",
    "    if return_model_param == True and annual_factor is None:\n",
    "        print(\"Regression assumes 'annual_factor' equals to 12 since it was not provided\")\n",
    "        annual_factor = 12\n",
    "    \n",
    "    if window_size is None:\n",
    "        print('Using \"window\" of 60 periods, since none were provided.')\n",
    "        window_size = 60\n",
    "\n",
    "    if y.shape[1] > 1:\n",
    "        raise ValueError('y must be a single column DataFrame or Series')\n",
    "\n",
    "    X = X.shift(lag_periods) # Lag the predictors\n",
    "\n",
    "    y_name = y.columns[0]\n",
    "    X_names = \" + \".join(list(X.columns))\n",
    "    X_names = \"Intercept + \" + X_names if intercept else X_names\n",
    "\n",
    "    # Add the intercept\n",
    "    if intercept:\n",
    "        X = sm.add_constant(X)\n",
    " \n",
    "    # Check if y and X have the same length\n",
    "    if len(X.index) != len(y.index):\n",
    "        print(f'y has lenght {len(y.index)} and X has lenght {len(X.index)}. Joining y and X by y.index...')\n",
    "        df = y.join(X, how='left')\n",
    "        df = df.dropna()\n",
    "        y = df[y_name]\n",
    "        X = df.drop(columns=y_name)\n",
    "        if len(X.index) < len(X.columns) + 1:\n",
    "            raise Exception('Indexes of y and X do not match and there are less observations than degrees of freedom. Cannot calculate regression')\n",
    "        \n",
    "\n",
    "    summary_pred = pd.DataFrame({})\t\n",
    "    \n",
    "    for idx in range(window_size, len(y.index)-lag_periods+1, 1):\n",
    "        prediction_date = y.index[idx+lag_periods-1]\n",
    "        \n",
    "        idx_start = idx - window_size if rolling else 0\n",
    "        y_in_sample = y.iloc[idx_start:idx].copy()\n",
    "        X_in_sample = X.iloc[idx_start:idx].copy()\n",
    "\n",
    "        X_out_sample = X.iloc[idx+lag_periods-1, :].copy()\n",
    "        y_out_sample = y.iloc[idx+lag_periods-1].copy()\n",
    "        \n",
    "        # Fit the regression model\n",
    "        try:\n",
    "            ols_model = sm.OLS(y_in_sample, X_in_sample, missing='drop')\n",
    "        except ValueError:\n",
    "            y = y.reset_index(drop=True)\n",
    "            X = X.reset_index(drop=True)\n",
    "            ols_model = sm.OLS(y, X, missing=\"drop\")\n",
    "            print(f'\"Reset indexes was required to make regression work. Try passing \"y\" and \"X\" as pd.DataFrame')\n",
    "        \n",
    "        ols_results = ols_model.fit()\n",
    "        \n",
    "        y_pred = ols_results.predict(X_out_sample)[0]\n",
    "        y_in_sample_mean = y_in_sample.mean().squeeze()\n",
    "        y_actual = y_out_sample.squeeze()\n",
    "        \n",
    "        summary_pred.loc[prediction_date,'Prediction'] = y_pred\n",
    "        summary_pred.loc[prediction_date,'Naive Prediction (Mean)'] = y_in_sample_mean\n",
    "        summary_pred.loc[prediction_date,'Actual'] = y_actual\n",
    "        \n",
    "        summary_pred.loc[prediction_date,'Prediction Error'] = summary_pred.loc[prediction_date,'Prediction'] - summary_pred.loc[prediction_date,'Actual']\n",
    "        summary_pred.loc[prediction_date,'Total Error'] = summary_pred.loc[prediction_date,'Naive Prediction (Mean)'] - summary_pred.loc[prediction_date,'Actual']\n",
    "\n",
    "\n",
    "    if return_model_param:\n",
    "        regression_statistics = pd.DataFrame(index=[y_name])\n",
    "\n",
    "        # Calculate/get statistics:\n",
    "        regression_params_names = []\n",
    "        if intercept == True:\n",
    "            regression_statistics.loc[y_name, 'Alpha'] = ols_results.params.iloc[0]\n",
    "            regression_statistics.loc[y_name, 'Annualized Alpha'] = ols_results.params.iloc[0] * annual_factor # Annualized Alpha\n",
    "            regression_statistics.loc[y_name, 'P-Value (Alpha)'] = ols_results.pvalues.iloc[0] # Alpha p-value\n",
    "            regression_params_names.append('Alpha')\n",
    "        else:\n",
    "            print('No intercept in regression. OOS R-Squared might not make statistical sense.')      \n",
    "        \n",
    "        X_names = list(X.columns[1:]) if intercept else list(X.columns)\n",
    "        betas = ols_results.params[1:] if intercept else ols_results.params\n",
    "        betas_p_values = ols_results.pvalues[1:] if intercept else ols_results.pvalues\n",
    "        \n",
    "        for idx in range(len(X_names)):\n",
    "            regression_statistics.loc[y_name, f\"Beta ({X_names[idx]})\"] = betas.iloc[idx] # Betas\n",
    "            regression_statistics.loc[y_name, f\"P-Value ({X_names[idx]})\"] = betas_p_values.iloc[idx] # Beta p-values\n",
    "            regression_params_names.append(X_names[idx])\n",
    "\n",
    "        rss = (np.array(summary_pred['Prediction Error']) ** 2).sum()\n",
    "        tss = (np.array(summary_pred['Total Error']) ** 2).sum()\n",
    "        oos_rsquared = 1 - rss / tss\n",
    "        tracking_error = np.sqrt(rss / len(summary_pred))\n",
    "\n",
    "        regression_statistics.loc[y_name, 'Tracking Error'] = tracking_error\n",
    "        regression_statistics.loc[y_name, 'Annualized Tracking Error'] = tracking_error * (annual_factor ** 0.5) # Annualized Residuals Volatility\n",
    "        regression_statistics.loc[y_name, 'OOS R-Squared'] = oos_rsquared # R-squared\n",
    "        \n",
    "        regression_statistics_t = regression_statistics.T\n",
    "\n",
    "        significant_params = [param for param in regression_params_names if regression_statistics_t.loc[f'P-Value ({param})', y_name] < significance_level]\n",
    "\n",
    "        if len(significant_params) > 0:\n",
    "            print(f'Significant parameters at a confidence level of {(1-significance_level):.1%}: {\", \".join(significant_params)}')\n",
    "        else:\n",
    "            print(f'No significant parameters at {(1-significance_level):.1%} level of confidence')\n",
    "        return filter_columns_and_indexes(\n",
    "            regression_statistics_t,\n",
    "            keep_columns=keep_columns,\n",
    "            drop_columns=drop_columns,\n",
    "            keep_indexes=keep_indexes,\n",
    "            drop_indexes=drop_indexes\n",
    "        )\n",
    "\n",
    "    return filter_columns_and_indexes(\n",
    "        summary_pred,\n",
    "        keep_columns=keep_columns,\n",
    "        drop_columns=drop_columns,\n",
    "        keep_indexes=keep_indexes,\n",
    "        drop_indexes=drop_indexes\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data in the file (sheets, columns, data):\n",
    "INFILE = \"data/final_exam_data_old.xlsx\"\n",
    "\n",
    "portfolio = pd.read_excel(INFILE, sheet_name = 'portfolio', index_col=0) ##weekly\n",
    "forecasting = pd.read_excel(INFILE, sheet_name = 'forecasting', index_col=0)  ##monthly\n",
    "fx_data = pd.read_excel(INFILE, sheet_name = 'fx_carry', index_col=0) ## daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tangency Portfolio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SPY</th>\n",
       "      <td>0.857595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BTC</th>\n",
       "      <td>0.141710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>USO</th>\n",
       "      <td>-0.043098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TLT</th>\n",
       "      <td>-0.040488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IEF</th>\n",
       "      <td>0.167449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IYR</th>\n",
       "      <td>-0.440830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GLD</th>\n",
       "      <td>0.357662</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Tangency Portfolio\n",
       "SPY            0.857595\n",
       "BTC            0.141710\n",
       "USO           -0.043098\n",
       "TLT           -0.040488\n",
       "IEF            0.167449\n",
       "IYR           -0.440830\n",
       "GLD            0.357662"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tangency Weights\n",
    "tangency_weights = calc_tangency_port(returns=portfolio)\n",
    "tangency_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tangency Portfolio (rescaled 13.0% p.a.)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SPY</th>\n",
       "      <td>0.479969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BTC</th>\n",
       "      <td>0.079311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>USO</th>\n",
       "      <td>-0.024120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TLT</th>\n",
       "      <td>-0.022660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IEF</th>\n",
       "      <td>0.093716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IYR</th>\n",
       "      <td>-0.246719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GLD</th>\n",
       "      <td>0.200172</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Tangency Portfolio (rescaled 13.0% p.a.)\n",
       "SPY                                  0.479969\n",
       "BTC                                  0.079311\n",
       "USO                                 -0.024120\n",
       "TLT                                 -0.022660\n",
       "IEF                                  0.093716\n",
       "IYR                                 -0.246719\n",
       "GLD                                  0.200172"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Target excess return\n",
    "TARGET_RET = 0.0025\n",
    "ANNUAL_FACTOR = 52\n",
    "mv_target_xs = calc_tangency_port(returns=portfolio,\n",
    "                               annual_factor=ANNUAL_FACTOR,\n",
    "                               target_return=TARGET_RET*ANNUAL_FACTOR)\n",
    "mv_target_xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GMV Portfolio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SPY</th>\n",
       "      <td>0.092076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BTC</th>\n",
       "      <td>-0.001009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>USO</th>\n",
       "      <td>0.002750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TLT</th>\n",
       "      <td>-0.476953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IEF</th>\n",
       "      <td>1.427028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IYR</th>\n",
       "      <td>-0.041140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GLD</th>\n",
       "      <td>-0.002752</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     GMV Portfolio\n",
       "SPY       0.092076\n",
       "BTC      -0.001009\n",
       "USO       0.002750\n",
       "TLT      -0.476953\n",
       "IEF       1.427028\n",
       "IYR      -0.041140\n",
       "GLD      -0.002752"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmv_weights = calc_gmv_port(returns=portfolio)\n",
    "gmv_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MV Portfolio (target 13.0%)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SPY</th>\n",
       "      <td>23.744975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BTC</th>\n",
       "      <td>4.408708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>USO</th>\n",
       "      <td>-1.413833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TLT</th>\n",
       "      <td>13.008858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IEF</th>\n",
       "      <td>-37.491263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IYR</th>\n",
       "      <td>-12.390710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GLD</th>\n",
       "      <td>11.133266</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     MV Portfolio (target 13.0%)\n",
       "SPY                    23.744975\n",
       "BTC                     4.408708\n",
       "USO                    -1.413833\n",
       "TLT                    13.008858\n",
       "IEF                   -37.491263\n",
       "IYR                   -12.390710\n",
       "GLD                    11.133266"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Target return (assuming total returns):\n",
    "mv_target = calc_mv_port(returns=portfolio,\n",
    "                         is_excess_returns=False,\n",
    "                         annual_factor=ANNUAL_FACTOR,\n",
    "                         target_return=TARGET_RET*ANNUAL_FACTOR)\n",
    "mv_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No risk-free rate provided. Interpret \"Sharpe\" as \"Mean/Volatility\".\n",
      "\n"
     ]
    }
   ],
   "source": [
    "excess_ret_wo_BTC = portfolio.drop(columns = {'BTC'})\n",
    "excess_ret_wo_BTC_IS = excess_ret_wo_BTC.loc[:'2021']\n",
    "excess_ret_wo_BTC_OOS = excess_ret_wo_BTC.loc['2022':'2023']\n",
    "\n",
    "tangency_weights_wo_BTC = calc_tangency_port(returns=excess_ret_wo_BTC_IS,\n",
    "                                            annual_factor=ANNUAL_FACTOR,\n",
    "                                            target_return=TARGET_RET*ANNUAL_FACTOR)\n",
    "\n",
    "equal_weights_wo_BTC = calc_equal_weights_port(returns=excess_ret_wo_BTC_IS,\n",
    "                                            annual_factor=ANNUAL_FACTOR,\n",
    "                                            target_return=TARGET_RET*ANNUAL_FACTOR)\n",
    "\n",
    "tangency_returns_wo_BTC = excess_ret_wo_BTC_OOS @ tangency_weights_wo_BTC\n",
    "ew_returns_wo_BTC = excess_ret_wo_BTC_OOS @ equal_weights_wo_BTC\n",
    "return_stats = calc_returns_statistics(returns=pd.concat([tangency_returns_wo_BTC, ew_returns_wo_BTC], axis=1),\n",
    "                                       annual_factor=ANNUAL_FACTOR,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
